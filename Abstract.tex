\begin{abstract}
The deployment of machine learning systems in safety-critical applications has highlighted two fundamental challenges: detecting when models encounter out-of-distribution (OOD) inputs and identifying when large language models generate hallucinated content. This dissertation proposes novel information-theoretic approaches to address both challenges through principled frameworks grounded in mutual information theory and representation learning.

For out-of-distribution detection, we develop methods that leverage mutual information between input features and learned representations to identify samples that fall outside the training distribution. Our approach provides theoretical guarantees about detection performance while maintaining computational efficiency across diverse domains including computer vision and natural language processing. We demonstrate that information-theoretic metrics can effectively distinguish between in-distribution and out-of-distribution samples without requiring access to OOD data during training.

For hallucination detection in large language models, we propose a contrastive mutual information estimation framework that analyzes information flow between intermediate layer representations during question-answering tasks. Our central hypothesis is that intrinsic hallucinations arise from a loss of mutual information between input queries and generated responses, particularly in the intermediate layers of foundation models. We develop a novel contrastive learning approach specifically designed for estimating mutual information between transformer layer representations, enabling real-time detection of potential hallucinations.

The proposed contrastive method addresses key limitations of existing mutual information estimation techniques by focusing on question-answer consistency across layers while scaling efficiently to large transformer models. We validate our approach on comprehensive benchmarks including Natural Questions, TriviaQA, HaluEval, and TruthfulQA, demonstrating superior performance compared to existing hallucination detection methods.

Our contributions include: (1) theoretical frameworks connecting information theory to both OOD detection and hallucination detection, (2) novel algorithms for mutual information estimation in high-dimensional neural network representations, (3) comprehensive experimental validation across multiple domains and model architectures, and (4) practical deployment considerations for safety-critical applications. This work establishes information theory as a unifying principle for understanding and addressing fundamental reliability challenges in modern machine learning systems.

\end{abstract}