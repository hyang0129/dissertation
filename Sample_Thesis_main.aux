\relax 
\@input{CoverPage.aux}
\@input{Abstract.aux}
\@input{Acknowledgements.aux}
\@input{Dedication.aux}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{yang2021generalized}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Definitions}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Out of Distribution Detection}{2}{}\protected@file@percent }
\newlabel{defineood}{{2.1.1}{2}{}{}{}}
\newlabel{methodmsp}{{2.1.2}{3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Anomaly Detection}{3}{}\protected@file@percent }
\citation{shannon1948mathematical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definiton and Scope}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training Assumptions}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Evaluation Settings}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Information Theory}{5}{}\protected@file@percent }
\newlabel{def:entropy}{{2.3.1}{5}{}{}{}}
\newlabel{def:conditional_entropy}{{2.3.2}{5}{}{}{}}
\newlabel{def:mutual_information}{{2.3.3}{5}{}{}{}}
\newlabel{def:kl_divergence}{{2.3.4}{5}{}{}{}}
\newlabel{def:chain_rule}{{2.3.5}{5}{}{}{}}
\newlabel{def:mi_kl}{{2.3.6}{5}{}{}{}}
\newlabel{def:nonneg_mi}{{2.3.7}{5}{}{}{}}
\newlabel{def:chain_rule_mi}{{2.3.8}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Entropy and Mutual Information}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Information Bottleneck and Minimal Sufficient Statistic}{6}{}\protected@file@percent }
\citation{tishby2000information}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Minimal Sufficient Statistic}{7}{}\protected@file@percent }
\newlabel{def:mss}{{2.4.1}{7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Information Bottleneck}{7}{}\protected@file@percent }
\newlabel{def:ib}{{2.4.2}{8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Dataset Domain}{8}{}\protected@file@percent }
\newlabel{def:singledomain}{{2.5.1}{10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Domain Features and Domain Feature Collapse}{10}{}\protected@file@percent }
\newlabel{def:domainfeatures}{{2.5.2}{10}{}{}{}}
\newlabel{def:domainfeaturecollapse}{{2.5.3}{10}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Unlabeled OOD Detection}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Large Language Models}{12}{}\protected@file@percent }
\citation{shannon1948mathematical}
\citation{linsker1988self}
\citation{hjelm2019learning}
\citation{oord2018representation}
\citation{tishby2000information}
\citation{alemi2017deep}
\citation{mcallester1999pac}
\citation{xu2017information}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Literature Review}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Information Theory in Machine Learning}{15}{}\protected@file@percent }
\citation{jordan1999introduction,kingma2014auto}
\citation{chen2016infogan}
\citation{peng2005feature}
\citation{runge2019detecting}
\citation{belghazi2018mutual}
\citation{saxe2019information}
\citation{tishby2000information}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Representation Learning}{16}{}\protected@file@percent }
\citation{pearson1901liii}
\citation{hyvarinen2000independent}
\citation{hinton2006reducing}
\citation{kingma2014auto,rezende2014stochastic}
\citation{goodfellow2014generative}
\citation{chen2016infogan}
\citation{ho2020denoising,song2020score}
\citation{chen2020simple,he2020momentum}
\citation{chen2020simple}
\citation{he2020momentum}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{devlin2018bert}
\citation{radford2018improving}
\citation{raffel2020exploring}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Unsupervised Representation Learning}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Self-Supervised Learning}{17}{}\protected@file@percent }
\citation{oord2018representation,hjelm2019learning}
\citation{hendrycks2016baseline}
\citation{liang2017enhancing}
\citation{liu2020energy}
\citation{lee2018simple}
\citation{sun2022out}
\citation{tack2020csi}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Out of Distribution Detection}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Classical and Training-Agnostic Approaches}{18}{}\protected@file@percent }
\citation{guille2024cadet}
\citation{sehwag2021ssd}
\citation{zhou2022rethinking}
\citation{liu2023unsupervised}
\citation{du2024does}
\citation{yangcan}
\citation{cifar10}
\citation{ILSVRC15}
\citation{yang2022openood}
\citation{yang2022openood,zhang2023openood}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Self-Supervised and Unsupervised OOD Detection}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Benchmarking and Evaluation}{19}{}\protected@file@percent }
\citation{zhang2021out}
\citation{ramanagopal2018failing}
\citation{ekim2024distribution}
\citation{zhang2021out}
\citation{cao2020benchmark}
\citation{narayanaswamy2023exploring}
\citation{ekim2024distribution}
\citation{saadati2024out}
\citation{kafunah2023out}
\citation{kim2021wafer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Single-Domain OOD Detection}{20}{}\protected@file@percent }
\citation{yangcan}
\citation{katz2022training}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Domain Adaptation and Transfer Learning}{21}{}\protected@file@percent }
\citation{lakshminarayanan2017simple}
\citation{pmlr-v235-xu24ae}
\citation{manakul2023selfcheckgpt,zhang2023sirens}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Multi-Stage and Ensemble Approaches}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Hallucination Detection}{22}{}\protected@file@percent }
\citation{manakul2023selfcheckgpt}
\citation{li2023halueval}
\citation{peng2023check,chern2023factool}
\citation{farquhar2024detecting}
\citation{burns2023discovering}
\citation{li2023halueval}
\citation{lin2022truthfulqa}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Taxonomy of Hallucination Detection Approaches}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Information-Theoretic Perspectives}{23}{}\protected@file@percent }
\citation{he2016deep}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Evaluation and Benchmarks}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Model Architectures}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Convolutional Neural Networks}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Transformers}{24}{}\protected@file@percent }
\citation{radford2021learning}
\citation{brown2020language}
\citation{devlin2018bert}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Foundation Models}{25}{}\protected@file@percent }
\citation{ramanagopal2018failing}
\citation{wang2021deep}
\citation{bakator2018deep}
\citation{krizhevsky2012imagenet}
\citation{drummond2006open}
\citation{sehwag2021ssd}
\citation{wang2023clipn}
\citation{sehwag2021ssd,tack2020csi,liu2023unsupervised,guille2024cadet,wang2023clipn}
\citation{hendrycks2016baseline}
\citation{fort2021exploring}
\citation{du2024does,du2024and}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Label Blindness in Unlabeled OOD Detection}{26}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{26}{}\protected@file@percent }
\citation{wang2023clipn,esmaeilpour2022zero}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\citation{fort2021exploring}
\citation{sehwag2021ssd}
\citation{hendrycks2019using}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces An example failure case by visualizing the heatmaps of the gradient of a unlabeled SimCLR trained ResNet \citep  {chen2020simple} using the GradCAM method \citep  {selvaraju2017grad}. The OOD detection task is to detect OOD facial expressions. In this case, the OOD detection method fails as justified by our theoretical work, where the representations do not exhibit a strong gradient in regions commonly associated with facial expressions (i.e., eyebrows, mouth, etc.).}}{28}{}\protected@file@percent }
\newlabel{fig:grad}{{4.1}{28}{}{}{}}
\citation{yang2021generalized}
\citation{sehwag2021ssd}
\citation{wang2023clipn}
\citation{shannon1948mathematical}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Preliminaries}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Labeled and Unlabeled Out-of-Distribution Detection}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Self-Supervised and Unsupervised Learning}{29}{}\protected@file@percent }
\citation{federici2020learning}
\citation{shwartz2023compress}
\citation{shwartz2023compress}
\citation{chen2020simple}
\citation{federici2020learning}
\newlabel{definesuff}{{4.2.1}{30}{}{}{}}
\newlabel{defineminsuff}{{4.2.2}{30}{}{}{}}
\newlabel{superfluous}{{4.2}{30}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Guaranteed OOD Detection Failure}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Label Blindness Theorem (Strict Label Blindness)}{31}{}\protected@file@percent }
\newlabel{mainbodygenloss}{{4.3.1}{31}{}{}{}}
\newlabel{mainbodyfilter}{{4.3.2}{32}{}{}{}}
\newlabel{mainbodyfailood}{{4.3.3}{32}{}{}{}}
\citation{shwartz2017opening}
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Implications of Strict Label Blindness in Real World Situations}{33}{}\protected@file@percent }
\newlabel{fano}{{4.3.4}{33}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Theoretical Implications}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Benchmarking for Label Blindness Failure}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Bootstrapping and the Adjacent OOD Benchmark}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Why Adjacent OOD is Safety-Critical to Almost All Real World Systems}{34}{}\protected@file@percent }
\newlabel{mainbodyoverlap}{{4.4.1}{34}{}{}{}}
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\citation{fang2022out}
\citation{fang2022out}
\citation{sehwag2021ssd,liu2023unsupervised,guille2024cadet}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Comparing Adjacent, Near, and Far OOD Benchmarks}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Implications for OOD from Unlabeled Data}{35}{}\protected@file@percent }
\citation{hendrycks2016baseline}
\citation{chen2020simple}
\citation{hendrycks2019using}
\citation{sehwag2021ssd}
\citation{sun2022out}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experimental Results}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Experimental Setup}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised Baseline.}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Self-supervised Baselines.}{36}{}\protected@file@percent }
\citation{liu2023unsupervised}
\citation{xiao2020likelihood}
\citation{wang2023clipn}
\citation{icmlface}
\citation{KrauseStarkDengFei-Fei_3DRR2013}
\@writefile{toc}{\contentsline {paragraph}{Unsupervised Baseline.}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Zero-shot Baseline.}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Adjacent OOD Datasets}{37}{}\protected@file@percent }
\citation{bossard14}
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\citation{sharma2018conceptual}
\citation{sehwag2021ssd,tack2020csi,liu2023unsupervised,guille2024cadet,wang2023clipn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Experimental Results}{38}{}\protected@file@percent }
\citation{sun2022out}
\citation{khosla2020supervised}
\citation{du2024does}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Results from experiments across various datasets and methods. Unlabeled methods perform poorly in adjacent OOD detection. CLIPN performance is due to labels present in the pretraining dataset. Higher AUROC and lower FPR is better.}}{39}{}\protected@file@percent }
\newlabel{tab:results}{{4.1}{39}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Impact of Label Blindness on Future Research}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Recommendations for Future OOD Detection Research}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusion}{40}{}\protected@file@percent }
\citation{ramanagopal2018failing}
\citation{wang2021deep}
\citation{bakator2018deep}
\citation{krizhevsky2012imagenet}
\citation{drummond2006open}
\citation{zhang2023openood}
\citation{cifar10}
\citation{deng2009imagenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Domain Feature Collapse in Single-Domain OOD Detection}{42}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{42}{}\protected@file@percent }
\citation{zhang2021out}
\citation{ekim2024distribution}
\citation{saadati2024out}
\citation{yang2023medmnist}
\citation{rock_data}
\citation{helber2019eurosat}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Problem Formulation}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Single-Domain Datasets and Domain Features}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Theoretical Analysis: Domain Feature Collapse}{44}{}\protected@file@percent }
\newlabel{thm:domainfeaturecollapse}{{5.3.1}{44}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Implications for OOD Detection}{45}{}\protected@file@percent }
\citation{sun2022out}
\citation{fort2021exploring}
\citation{yang2022openood,zhang2023openood}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Domain Filtering: A Solution to Domain Feature Collapse}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Two-Stage Detector: Domain Filtering + OOD Detection}{46}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Two-Stage Domain Filtering for OOD Detection}}{46}{}\protected@file@percent }
\newlabel{alg:domainfiltering}{{1}{46}{}{}{}}
\citation{yangcan}
\citation{AIPlanet_DataSprint107_2024}
\citation{card_data}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Relationship to Near, Far, and Adjacent OOD}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Validation}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Domain Bench: Single-Domain Datasets}{47}{}\protected@file@percent }
\citation{yang2023medmnist}
\citation{helber2019eurosat}
\citation{fashion}
\citation{food}
\citation{single2023realwaste}
\citation{plant}
\citation{rock_data}
\citation{yang2023medmnist}
\citation{yoga_data}
\citation{zhang2023openood}
\citation{lecun1998gradient}
\citation{netzer2011reading}
\citation{cimpoi2014describing}
\citation{zhou2017places}
\citation{cifar10}
\citation{yangcan}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Experimental Setup}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Results and Analysis}{49}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Summary OOD Performance Across All Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score). We exclude the Rock dataset from this summary as it is an outlier for reasons explained in Section \ref {discussion}. Best scores are in bold and second best are bold and italicized. The domain filter methods are italicized. SC Resnet is not compatible with OOD methods that use logits.}}{49}{}\protected@file@percent }
\newlabel{tab:domain_collapse_results}{{5.1}{49}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Detailed Results by Dataset}{50}{}\protected@file@percent }
\citation{rock_data}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Summary FPR@95 OOD Performance Across Selected ID Datasets Reported As (In-Domain OOD Score)/(Out-of-Domain OOD Score). Best scores are in bold and second best are bold and italicized. Domain filtering methods are italicized.}}{51}{}\protected@file@percent }
\newlabel{tab:fpr95_selected}{{5.2}{51}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Case Study: Colon Dataset}{51}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Detailed FPR@95 OOD Detection Performance for the Colon Dataset. Results show performance across different out-of-domain test sets. Domain filtering methods are italicized.}}{52}{}\protected@file@percent }
\newlabel{tab:colon_detailed}{{5.3}{52}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}Discussion}{52}{}\protected@file@percent }
\newlabel{discussion}{{5.5.6}{52}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Rock Dataset as an Outlier}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Limitations and Future Work}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Assumptions and Scope}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Generalization to Other Domains}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Alternative Solutions}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusion}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Are Hallucinations Out of Distribution?}{56}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Research Timeline}{57}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{iclr2025_conference}
\bibdata{Bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{58}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{AIPlanet_DataSprint107_2024}{{1}{2023}{{AIPlanet}}{{}}}
\bibcite{alemi2017deep}{{2}{2017}{{Alemi et~al.}}{{Alemi, Fischer, Dillon, and Murphy}}}
\bibcite{bakator2018deep}{{3}{2018}{{Bakator \& Radosav}}{{Bakator and Radosav}}}
\bibcite{belghazi2018mutual}{{4}{2018}{{Belghazi et~al.}}{{Belghazi, Baratin, Rajeshwar, Ozair, Bengio, Courville, and Hjelm}}}
\bibcite{bossard14}{{5}{2014{a}}{{Bossard et~al.}}{{Bossard, Guillaumin, and Van~Gool}}}
\bibcite{food}{{6}{2014{b}}{{Bossard et~al.}}{{Bossard, Guillaumin, and Van~Gool}}}
\bibcite{brown2020language}{{7}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{burns2023discovering}{{8}{2023}{{Burns et~al.}}{{Burns, Ye, Klein, and Steinhardt}}}
\bibcite{cao2020benchmark}{{9}{2020}{{Cao et~al.}}{{Cao, Huang, Hui, and Cohen}}}
\bibcite{chen2020simple}{{10}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{chen2016infogan}{{11}{2016}{{Chen et~al.}}{{Chen, Duan, Houthooft, Schulman, Sutskever, and Abbeel}}}
\bibcite{chern2023factool}{{12}{2023}{{Chern et~al.}}{{Chern, Chern, Chen, Qian, Wei, Zou, and Graham}}}
\bibcite{cimpoi2014describing}{{13}{2014}{{Cimpoi et~al.}}{{Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi}}}
\bibcite{deng2009imagenet}{{14}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{devlin2018bert}{{15}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dosovitskiy2020image}{{16}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}}}
\bibcite{drummond2006open}{{17}{2006}{{Drummond \& Shearer}}{{Drummond and Shearer}}}
\bibcite{du2024does}{{18}{2024{a}}{{Du et~al.}}{{Du, Fang, Diakonikolas, and Li}}}
\bibcite{du2024and}{{19}{2024{b}}{{Du et~al.}}{{Du, Sun, and Li}}}
\bibcite{ekim2024distribution}{{20}{2024}{{Ekim et~al.}}{{Ekim, Tadesse, Robinson, Hacheme, Schmitt, Dodhia, and Ferres}}}
\bibcite{icmlface}{{21}{2013}{{Erhan et~al.}}{{Erhan, Goodfellow, Cukierski, and Bengio}}}
\bibcite{esmaeilpour2022zero}{{22}{2022}{{Esmaeilpour et~al.}}{{Esmaeilpour, Liu, Robertson, and Shu}}}
\bibcite{fang2022out}{{23}{2022}{{Fang et~al.}}{{Fang, Li, Lu, Dong, Han, and Liu}}}
\bibcite{farquhar2024detecting}{{24}{2024}{{Farquhar et~al.}}{{Farquhar, Kossen, Kuhn, and Gal}}}
\bibcite{federici2020learning}{{25}{2020}{{Federici et~al.}}{{Federici, Dutta, Forr{\'e}, Kushman, and Akata}}}
\bibcite{fort2021exploring}{{26}{2021}{{Fort et~al.}}{{Fort, Ren, and Lakshminarayanan}}}
\bibcite{goodfellow2014generative}{{27}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{guille2024cadet}{{28}{2024}{{Guille-Escuret et~al.}}{{Guille-Escuret, Rodriguez, Vazquez, Mitliagkas, and Monteiro}}}
\bibcite{he2016deep}{{29}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2020momentum}{{30}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{helber2019eurosat}{{31}{2019}{{Helber et~al.}}{{Helber, Bischke, Dengel, and Borth}}}
\bibcite{hendrycks2016baseline}{{32}{2016}{{Hendrycks \& Gimpel}}{{Hendrycks and Gimpel}}}
\bibcite{hendrycks2019using}{{33}{2019}{{Hendrycks et~al.}}{{Hendrycks, Mazeika, Kadavath, and Song}}}
\bibcite{hinton2006reducing}{{34}{2006}{{Hinton \& Salakhutdinov}}{{Hinton and Salakhutdinov}}}
\bibcite{hjelm2019learning}{{35}{2019}{{Hjelm et~al.}}{{Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman, Trischler, and Bengio}}}
\bibcite{ho2020denoising}{{36}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{rock_data}{{37}{2021}{{Hossain et~al.}}{{Hossain, Uddin, Nahin, and Ibne~Eunus}}}
\bibcite{plant}{{38}{2015}{{Hughes \& Salath{\'{e} }}}{{Hughes and Salath{\'{e} }}}}
\bibcite{hyvarinen2000independent}{{39}{2000}{{Hyv{\"a}rinen \& Oja}}{{Hyv{\"a}rinen and Oja}}}
\bibcite{jordan1999introduction}{{40}{1999}{{Jordan et~al.}}{{Jordan, Ghahramani, Jaakkola, and Saul}}}
\bibcite{kafunah2023out}{{41}{2023}{{Kafunah et~al.}}{{Kafunah, Verma, Ali, and Breslin}}}
\bibcite{katz2022training}{{42}{2022}{{Katz-Samuels et~al.}}{{Katz-Samuels, Nakhleh, Nowak, and Li}}}
\bibcite{khosla2020supervised}{{43}{2020}{{Khosla et~al.}}{{Khosla, Teterwak, Wang, Sarna, Tian, Isola, Maschinot, Liu, and Krishnan}}}
\bibcite{kim2021wafer}{{44}{2021}{{Kim et~al.}}{{Kim, Cho, and Lee}}}
\bibcite{kingma2014auto}{{45}{2014}{{Kingma \& Welling}}{{Kingma and Welling}}}
\bibcite{KrauseStarkDengFei-Fei_3DRR2013}{{46}{2013}{{Krause et~al.}}{{Krause, Stark, Deng, and Fei-Fei}}}
\bibcite{cifar10}{{47}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Nair, and Hinton}}}
\bibcite{krizhevsky2012imagenet}{{48}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lakshminarayanan2017simple}{{49}{2017}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel, and Blundell}}}
\bibcite{lecun1998gradient}{{50}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{lee2018simple}{{51}{2018}{{Lee et~al.}}{{Lee, Lee, Lee, and Shin}}}
\bibcite{li2023halueval}{{52}{2023}{{Li et~al.}}{{Li, Cheng, Zhao, Nie, and Wen}}}
\bibcite{liang2017enhancing}{{53}{2017}{{Liang et~al.}}{{Liang, Li, and Srikant}}}
\bibcite{lin2022truthfulqa}{{54}{2022}{{Lin et~al.}}{{Lin, Hilton, and Evans}}}
\bibcite{linsker1988self}{{55}{1988}{{Linsker}}{{}}}
\bibcite{liu2020energy}{{56}{2020}{{Liu et~al.}}{{Liu, Wang, Owens, and Li}}}
\bibcite{liu2023unsupervised}{{57}{2023}{{Liu et~al.}}{{Liu, Zhou, Wang, and Weinberger}}}
\bibcite{manakul2023selfcheckgpt}{{58}{2023}{{Manakul et~al.}}{{Manakul, Liusie, and Gales}}}
\bibcite{mcallester1999pac}{{59}{1999}{{McAllester}}{{}}}
\bibcite{mikolov2013efficient}{{60}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{narayanaswamy2023exploring}{{61}{2023}{{Narayanaswamy et~al.}}{{Narayanaswamy, Mubarka, Anirudh, Rajan, and Thiagarajan}}}
\bibcite{netzer2011reading}{{62}{2011}{{Netzer et~al.}}{{Netzer, Wang, Coates, Bissacco, Wu, and Ng}}}
\bibcite{pearson1901liii}{{63}{1901}{{Pearson}}{{}}}
\bibcite{peng2023check}{{64}{2023}{{Peng et~al.}}{{Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.}}}
\bibcite{peng2005feature}{{65}{2005}{{Peng et~al.}}{{Peng, Long, and Ding}}}
\bibcite{pennington2014glove}{{66}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{radford2018improving}{{67}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, Sutskever, et~al.}}}
\bibcite{radford2021learning}{{68}{2021}{{Radford et~al.}}{{Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.}}}
\bibcite{raffel2020exploring}{{69}{2020}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, Liu, et~al.}}}
\bibcite{ramanagopal2018failing}{{70}{2018}{{Ramanagopal et~al.}}{{Ramanagopal, Anderson, Vasudevan, and Johnson-Roberson}}}
\bibcite{rezende2014stochastic}{{71}{2014}{{Rezende et~al.}}{{Rezende, Mohamed, and Wierstra}}}
\bibcite{runge2019detecting}{{72}{2019}{{Runge et~al.}}{{Runge, Nowack, Kretschmer, Flaxman, and Sejdinovic}}}
\bibcite{ILSVRC15}{{73}{2015}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei}}}
\bibcite{saadati2024out}{{74}{2024}{{Saadati et~al.}}{{Saadati, Balu, Chiranjeevi, Jubery, Singh, Sarkar, Singh, and Ganapathysubramanian}}}
\bibcite{saxe2019information}{{75}{2019}{{Saxe et~al.}}{{Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey, and Cox}}}
\bibcite{sehwag2021ssd}{{76}{2021}{{Sehwag et~al.}}{{Sehwag, Chiang, and Mittal}}}
\bibcite{selvaraju2017grad}{{77}{2017}{{Selvaraju et~al.}}{{Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra}}}
\bibcite{shannon1948mathematical}{{78}{1948}{{Shannon}}{{}}}
\bibcite{sharma2018conceptual}{{79}{2018}{{Sharma et~al.}}{{Sharma, Ding, Goodman, and Soricut}}}
\bibcite{shwartz2023compress}{{80}{2023}{{Shwartz-Ziv \& LeCun}}{{Shwartz-Ziv and LeCun}}}
\bibcite{shwartz2017opening}{{81}{2017}{{Shwartz-Ziv \& Tishby}}{{Shwartz-Ziv and Tishby}}}
\bibcite{single2023realwaste}{{82}{2023}{{Single et~al.}}{{Single, Jain, and Jain}}}
\bibcite{song2020score}{{83}{2021}{{Song et~al.}}{{Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole}}}
\bibcite{card_data}{{84}{2020}{{Soni}}{{}}}
\bibcite{yoga_data}{{85}{2020}{{sumanthvrao}}{{}}}
\bibcite{sun2022out}{{86}{2022}{{Sun et~al.}}{{Sun, Ming, Zhu, and Li}}}
\bibcite{tack2020csi}{{87}{2020}{{Tack et~al.}}{{Tack, Mo, Jeong, and Shin}}}
\bibcite{tishby2000information}{{88}{2000}{{Tishby et~al.}}{{Tishby, Pereira, and Bialek}}}
\bibcite{oord2018representation}{{89}{2018}{{van~den Oord et~al.}}{{van~den Oord, Li, and Vinyals}}}
\bibcite{vaswani2017attention}{{90}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2023clipn}{{91}{2023}{{Wang et~al.}}{{Wang, Li, Yao, and Li}}}
\bibcite{wang2021deep}{{92}{2021}{{Wang \& Deng}}{{Wang and Deng}}}
\bibcite{fashion}{{93}{2017}{{Xiao et~al.}}{{Xiao, Rasul, and Vollgraf}}}
\bibcite{xiao2020likelihood}{{94}{2020}{{Xiao et~al.}}{{Xiao, Yan, and Amit}}}
\bibcite{xu2017information}{{95}{2017}{{Xu \& Raginsky}}{{Xu and Raginsky}}}
\bibcite{pmlr-v235-xu24ae}{{96}{2024}{{Xu et~al.}}{{Xu, Yu, Xu, Inkawhich, and Chen}}}
\bibcite{yangcan}{{97}{2025}{{Yang et~al.}}{{Yang, Yu, and Desell}}}
\bibcite{yang2023medmnist}{{98}{2023}{{Yang et~al.}}{{Yang, Shi, Wei, Liu, Zhao, Ke, Pfister, and Ni}}}
\bibcite{yang2021generalized}{{99}{2021}{{Yang et~al.}}{{Yang, Zhou, Li, and Liu}}}
\bibcite{yang2022openood}{{100}{2022}{{Yang et~al.}}{{Yang, Wang, Zou, Zhou, Ding, Peng, Wang, Chen, Li, Sun, et~al.}}}
\bibcite{zhang2023sirens}{{101}{2023{a}}{{Zhang et~al.}}{{Zhang, Li, Zhao, Xu, et~al.}}}
\bibcite{zhang2023openood}{{102}{2023{b}}{{Zhang et~al.}}{{Zhang, Yang, Wang, Wang, Lin, Zhang, Sun, Du, Li, Liu, et~al.}}}
\bibcite{zhang2021out}{{103}{2021}{{Zhang et~al.}}{{Zhang, Delbrouck, and Rubin}}}
\bibcite{zhou2017places}{{104}{2017}{{Zhou et~al.}}{{Zhou, Lapedriza, Khosla, Oliva, and Torralba}}}
\bibcite{zhou2022rethinking}{{105}{2022}{{Zhou}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{69}{}\protected@file@percent }
\@input{AppendixA.aux}
\gdef \@abspage@last{96}
