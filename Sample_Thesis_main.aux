\relax 
\@input{CoverPage.aux}
\@input{Abstract.aux}
\@input{Acknowledgements.aux}
\@input{Dedication.aux}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{yang2021generalized}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Definitions}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Out of Distribution Detection}{2}{}\protected@file@percent }
\newlabel{defineood}{{2.1.1}{2}{}{}{}}
\newlabel{methodmsp}{{2.1.2}{3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Anomaly Detection}{3}{}\protected@file@percent }
\citation{shannon1948mathematical}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definiton and Scope}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training Assumptions}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Evaluation Settings}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Information Theory}{5}{}\protected@file@percent }
\newlabel{def:entropy}{{2.3.1}{5}{}{}{}}
\newlabel{def:conditional_entropy}{{2.3.2}{5}{}{}{}}
\newlabel{def:mutual_information}{{2.3.3}{5}{}{}{}}
\newlabel{def:kl_divergence}{{2.3.4}{5}{}{}{}}
\newlabel{def:chain_rule}{{2.3.5}{5}{}{}{}}
\newlabel{def:mi_kl}{{2.3.6}{5}{}{}{}}
\newlabel{def:nonneg_mi}{{2.3.7}{5}{}{}{}}
\newlabel{def:chain_rule_mi}{{2.3.8}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Entropy and Mutual Information}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Information Bottleneck and Minimal Sufficient Statistic}{6}{}\protected@file@percent }
\citation{tishby2000information}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Minimal Sufficient Statistic}{7}{}\protected@file@percent }
\newlabel{def:mss}{{2.4.1}{7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Information Bottleneck}{7}{}\protected@file@percent }
\newlabel{def:ib}{{2.4.2}{8}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Dataset Domain}{8}{}\protected@file@percent }
\newlabel{def:singledomain}{{2.5.1}{10}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Unlabeled OOD Detection}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Large Language Models}{11}{}\protected@file@percent }
\citation{shannon1948mathematical}
\citation{linsker1988self}
\citation{hjelm2019learning}
\citation{oord2018representation}
\citation{tishby2000information}
\citation{alemi2017deep}
\citation{mcallester1999pac}
\citation{xu2017information}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Literature Review}{14}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Information Theory in Machine Learning}{14}{}\protected@file@percent }
\citation{jordan1999introduction,kingma2014auto}
\citation{chen2016infogan}
\citation{peng2005feature}
\citation{runge2019detecting}
\citation{belghazi2018mutual}
\citation{saxe2019information}
\citation{tishby2000information}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Representation Learning}{15}{}\protected@file@percent }
\citation{pearson1901liii}
\citation{hyvarinen2000independent}
\citation{hinton2006reducing}
\citation{kingma2014auto,rezende2014stochastic}
\citation{goodfellow2014generative}
\citation{chen2016infogan}
\citation{ho2020denoising,song2020score}
\citation{chen2020simple,he2020momentum}
\citation{chen2020simple}
\citation{he2020momentum}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{devlin2018bert}
\citation{radford2018improving}
\citation{raffel2020exploring}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Unsupervised Representation Learning}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Self-Supervised Learning}{16}{}\protected@file@percent }
\citation{oord2018representation,hjelm2019learning}
\citation{hendrycks2016baseline}
\citation{liang2017enhancing}
\citation{liu2020energy}
\citation{lee2018simple}
\citation{sun2022out}
\citation{tack2020csi}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Out of Distribution Detection}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Classical and Training-Agnostic Approaches}{17}{}\protected@file@percent }
\citation{guille2024cadet}
\citation{sehwag2021ssd}
\citation{zhou2022rethinking}
\citation{liu2023unsupervised}
\citation{du2024does}
\citation{yangcan}
\citation{cifar10}
\citation{ILSVRC15}
\citation{yang2022openood}
\citation{yang2022openood,zhang2023openood}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Self-Supervised and Unsupervised OOD Detection}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Benchmarking and Evaluation}{18}{}\protected@file@percent }
\citation{zhang2021out}
\citation{ramanagopal2018failing}
\citation{ekim2024distribution}
\citation{manakul2023selfcheckgpt,zhang2023sirens}
\citation{manakul2023selfcheckgpt}
\citation{li2023halueval}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Hallucination Detection}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Taxonomy of Hallucination Detection Approaches}{19}{}\protected@file@percent }
\citation{peng2023check,chern2023factool}
\citation{farquhar2024detecting}
\citation{burns2023discovering}
\citation{li2023halueval}
\citation{lin2022truthfulqa}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Information-Theoretic Perspectives}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Evaluation and Benchmarks}{20}{}\protected@file@percent }
\citation{he2016deep}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{radford2021learning}
\citation{brown2020language}
\citation{devlin2018bert}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Model Architectures}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Convolutional Neural Networks}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Transformers}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Foundation Models}{21}{}\protected@file@percent }
\citation{ramanagopal2018failing}
\citation{wang2021deep}
\citation{bakator2018deep}
\citation{krizhevsky2012imagenet}
\citation{drummond2006open}
\citation{sehwag2021ssd}
\citation{wang2023clipn}
\citation{sehwag2021ssd,tack2020csi,liu2023unsupervised,guille2024cadet,wang2023clipn}
\citation{hendrycks2016baseline}
\citation{fort2021exploring}
\citation{du2024does,du2024and}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Label Blindness in Unlabeled OOD Detection}{22}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{22}{}\protected@file@percent }
\citation{wang2023clipn,esmaeilpour2022zero}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\citation{chen2020simple}
\citation{selvaraju2017grad}
\citation{fort2021exploring}
\citation{sehwag2021ssd}
\citation{hendrycks2019using}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces An example failure case by visualizing the heatmaps of the gradient of a unlabeled SimCLR trained ResNet \citep  {chen2020simple} using the GradCAM method \citep  {selvaraju2017grad}. The OOD detection task is to detect OOD facial expressions. In this case, the OOD detection method fails as justified by our theoretical work, where the representations do not exhibit a strong gradient in regions commonly associated with facial expressions (i.e., eyebrows, mouth, etc.).}}{24}{}\protected@file@percent }
\newlabel{fig:grad}{{4.1}{24}{}{}{}}
\citation{yang2021generalized}
\citation{sehwag2021ssd}
\citation{wang2023clipn}
\citation{shannon1948mathematical}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Preliminaries}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Labeled and Unlabeled Out-of-Distribution Detection}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Self-Supervised and Unsupervised Learning}{25}{}\protected@file@percent }
\citation{federici2020learning}
\citation{shwartz2023compress}
\citation{shwartz2023compress}
\citation{chen2020simple}
\citation{federici2020learning}
\newlabel{definesuff}{{4.2.1}{26}{}{}{}}
\newlabel{defineminsuff}{{4.2.2}{26}{}{}{}}
\newlabel{superfluous}{{4.2}{26}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Guaranteed OOD Detection Failure}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Label Blindness Theorem (Strict Label Blindness)}{27}{}\protected@file@percent }
\newlabel{mainbodygenloss}{{4.3.1}{27}{}{}{}}
\newlabel{mainbodyfilter}{{4.3.2}{28}{}{}{}}
\newlabel{mainbodyfailood}{{4.3.3}{28}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Implications of Strict Label Blindness in Real World Situations}{28}{}\protected@file@percent }
\citation{shwartz2017opening}
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\newlabel{fano}{{4.3.4}{29}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Theoretical Implications}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Benchmarking for Label Blindness Failure}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Bootstrapping and the Adjacent OOD Benchmark}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Why Adjacent OOD is Safety-Critical to Almost All Real World Systems}{30}{}\protected@file@percent }
\newlabel{mainbodyoverlap}{{4.4.1}{30}{}{}{}}
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\citation{fang2022out}
\citation{fang2022out}
\citation{sehwag2021ssd,liu2023unsupervised,guille2024cadet}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Comparing Adjacent, Near, and Far OOD Benchmarks}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Implications for OOD from Unlabeled Data}{31}{}\protected@file@percent }
\citation{hendrycks2016baseline}
\citation{chen2020simple}
\citation{hendrycks2019using}
\citation{sehwag2021ssd}
\citation{sun2022out}
\citation{liu2023unsupervised}
\citation{xiao2020likelihood}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experimental Results}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Experimental Setup}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervised Baseline.}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Self-supervised Baselines.}{32}{}\protected@file@percent }
\citation{wang2023clipn}
\citation{icmlface}
\citation{KrauseStarkDengFei-Fei_3DRR2013}
\citation{bossard14}
\@writefile{toc}{\contentsline {paragraph}{Unsupervised Baseline.}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Zero-shot Baseline.}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Adjacent OOD Datasets}{33}{}\protected@file@percent }
\citation{sehwag2021ssd,hendrycks2019using,liu2023unsupervised}
\citation{sharma2018conceptual}
\citation{sehwag2021ssd,tack2020csi,liu2023unsupervised,guille2024cadet,wang2023clipn}
\citation{sun2022out}
\citation{khosla2020supervised}
\citation{du2024does}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Experimental Results}{34}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Results from experiments across various datasets and methods. Unlabeled methods perform poorly in adjacent OOD detection. CLIPN performance is due to labels present in the pretraining dataset. Higher AUROC and lower FPR is better.}}{35}{}\protected@file@percent }
\newlabel{tab:results}{{4.1}{35}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Impact of Label Blindness on Future Research}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Recommendations for Future OOD Detection Research}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusion}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Domain Feature Collapse: How Single Domain Training Removes Domain Features}{38}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Are Hallucinations Out of Distribution?}{39}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Research Timeline}{40}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{iclr2025_conference}
\bibdata{Bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Discussion}{41}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{alemi2017deep}{{1}{2017}{{Alemi et~al.}}{{Alemi, Fischer, Dillon, and Murphy}}}
\bibcite{bakator2018deep}{{2}{2018}{{Bakator \& Radosav}}{{Bakator and Radosav}}}
\bibcite{belghazi2018mutual}{{3}{2018}{{Belghazi et~al.}}{{Belghazi, Baratin, Rajeshwar, Ozair, Bengio, Courville, and Hjelm}}}
\bibcite{bossard14}{{4}{2014}{{Bossard et~al.}}{{Bossard, Guillaumin, and Van~Gool}}}
\bibcite{brown2020language}{{5}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{burns2023discovering}{{6}{2023}{{Burns et~al.}}{{Burns, Ye, Klein, and Steinhardt}}}
\bibcite{chen2020simple}{{7}{2020}{{Chen et~al.}}{{Chen, Kornblith, Norouzi, and Hinton}}}
\bibcite{chen2016infogan}{{8}{2016}{{Chen et~al.}}{{Chen, Duan, Houthooft, Schulman, Sutskever, and Abbeel}}}
\bibcite{chern2023factool}{{9}{2023}{{Chern et~al.}}{{Chern, Chern, Chen, Qian, Wei, Zou, and Graham}}}
\bibcite{devlin2018bert}{{10}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{dosovitskiy2020image}{{11}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}}}
\bibcite{drummond2006open}{{12}{2006}{{Drummond \& Shearer}}{{Drummond and Shearer}}}
\bibcite{du2024does}{{13}{2024{a}}{{Du et~al.}}{{Du, Fang, Diakonikolas, and Li}}}
\bibcite{du2024and}{{14}{2024{b}}{{Du et~al.}}{{Du, Sun, and Li}}}
\bibcite{ekim2024distribution}{{15}{2024}{{Ekim et~al.}}{{Ekim, Tadesse, Robinson, Hacheme, Schmitt, Dodhia, and Ferres}}}
\bibcite{icmlface}{{16}{2013}{{Erhan et~al.}}{{Erhan, Goodfellow, Cukierski, and Bengio}}}
\bibcite{esmaeilpour2022zero}{{17}{2022}{{Esmaeilpour et~al.}}{{Esmaeilpour, Liu, Robertson, and Shu}}}
\bibcite{fang2022out}{{18}{2022}{{Fang et~al.}}{{Fang, Li, Lu, Dong, Han, and Liu}}}
\bibcite{farquhar2024detecting}{{19}{2024}{{Farquhar et~al.}}{{Farquhar, Kossen, Kuhn, and Gal}}}
\bibcite{federici2020learning}{{20}{2020}{{Federici et~al.}}{{Federici, Dutta, Forr{\'e}, Kushman, and Akata}}}
\bibcite{fort2021exploring}{{21}{2021}{{Fort et~al.}}{{Fort, Ren, and Lakshminarayanan}}}
\bibcite{goodfellow2014generative}{{22}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{guille2024cadet}{{23}{2024}{{Guille-Escuret et~al.}}{{Guille-Escuret, Rodriguez, Vazquez, Mitliagkas, and Monteiro}}}
\bibcite{he2016deep}{{24}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2020momentum}{{25}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{hendrycks2016baseline}{{26}{2016}{{Hendrycks \& Gimpel}}{{Hendrycks and Gimpel}}}
\bibcite{hendrycks2019using}{{27}{2019}{{Hendrycks et~al.}}{{Hendrycks, Mazeika, Kadavath, and Song}}}
\bibcite{hinton2006reducing}{{28}{2006}{{Hinton \& Salakhutdinov}}{{Hinton and Salakhutdinov}}}
\bibcite{hjelm2019learning}{{29}{2019}{{Hjelm et~al.}}{{Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman, Trischler, and Bengio}}}
\bibcite{ho2020denoising}{{30}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{hyvarinen2000independent}{{31}{2000}{{Hyv{\"a}rinen \& Oja}}{{Hyv{\"a}rinen and Oja}}}
\bibcite{jordan1999introduction}{{32}{1999}{{Jordan et~al.}}{{Jordan, Ghahramani, Jaakkola, and Saul}}}
\bibcite{khosla2020supervised}{{33}{2020}{{Khosla et~al.}}{{Khosla, Teterwak, Wang, Sarna, Tian, Isola, Maschinot, Liu, and Krishnan}}}
\bibcite{kingma2014auto}{{34}{2014}{{Kingma \& Welling}}{{Kingma and Welling}}}
\bibcite{KrauseStarkDengFei-Fei_3DRR2013}{{35}{2013}{{Krause et~al.}}{{Krause, Stark, Deng, and Fei-Fei}}}
\bibcite{cifar10}{{36}{2009}{{Krizhevsky et~al.}}{{Krizhevsky, Nair, and Hinton}}}
\bibcite{krizhevsky2012imagenet}{{37}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lee2018simple}{{38}{2018}{{Lee et~al.}}{{Lee, Lee, Lee, and Shin}}}
\bibcite{li2023halueval}{{39}{2023}{{Li et~al.}}{{Li, Cheng, Zhao, Nie, and Wen}}}
\bibcite{liang2017enhancing}{{40}{2017}{{Liang et~al.}}{{Liang, Li, and Srikant}}}
\bibcite{lin2022truthfulqa}{{41}{2022}{{Lin et~al.}}{{Lin, Hilton, and Evans}}}
\bibcite{linsker1988self}{{42}{1988}{{Linsker}}{{}}}
\bibcite{liu2020energy}{{43}{2020}{{Liu et~al.}}{{Liu, Wang, Owens, and Li}}}
\bibcite{liu2023unsupervised}{{44}{2023}{{Liu et~al.}}{{Liu, Zhou, Wang, and Weinberger}}}
\bibcite{manakul2023selfcheckgpt}{{45}{2023}{{Manakul et~al.}}{{Manakul, Liusie, and Gales}}}
\bibcite{mcallester1999pac}{{46}{1999}{{McAllester}}{{}}}
\bibcite{mikolov2013efficient}{{47}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{pearson1901liii}{{48}{1901}{{Pearson}}{{}}}
\bibcite{peng2023check}{{49}{2023}{{Peng et~al.}}{{Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu, Chen, et~al.}}}
\bibcite{peng2005feature}{{50}{2005}{{Peng et~al.}}{{Peng, Long, and Ding}}}
\bibcite{pennington2014glove}{{51}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{radford2018improving}{{52}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, Sutskever, et~al.}}}
\bibcite{radford2021learning}{{53}{2021}{{Radford et~al.}}{{Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.}}}
\bibcite{raffel2020exploring}{{54}{2020}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, Liu, et~al.}}}
\bibcite{ramanagopal2018failing}{{55}{2018}{{Ramanagopal et~al.}}{{Ramanagopal, Anderson, Vasudevan, and Johnson-Roberson}}}
\bibcite{rezende2014stochastic}{{56}{2014}{{Rezende et~al.}}{{Rezende, Mohamed, and Wierstra}}}
\bibcite{runge2019detecting}{{57}{2019}{{Runge et~al.}}{{Runge, Nowack, Kretschmer, Flaxman, and Sejdinovic}}}
\bibcite{ILSVRC15}{{58}{2015}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei}}}
\bibcite{saxe2019information}{{59}{2019}{{Saxe et~al.}}{{Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey, and Cox}}}
\bibcite{sehwag2021ssd}{{60}{2021}{{Sehwag et~al.}}{{Sehwag, Chiang, and Mittal}}}
\bibcite{selvaraju2017grad}{{61}{2017}{{Selvaraju et~al.}}{{Selvaraju, Cogswell, Das, Vedantam, Parikh, and Batra}}}
\bibcite{shannon1948mathematical}{{62}{1948}{{Shannon}}{{}}}
\bibcite{sharma2018conceptual}{{63}{2018}{{Sharma et~al.}}{{Sharma, Ding, Goodman, and Soricut}}}
\bibcite{shwartz2023compress}{{64}{2023}{{Shwartz-Ziv \& LeCun}}{{Shwartz-Ziv and LeCun}}}
\bibcite{shwartz2017opening}{{65}{2017}{{Shwartz-Ziv \& Tishby}}{{Shwartz-Ziv and Tishby}}}
\bibcite{song2020score}{{66}{2021}{{Song et~al.}}{{Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole}}}
\bibcite{sun2022out}{{67}{2022}{{Sun et~al.}}{{Sun, Ming, Zhu, and Li}}}
\bibcite{tack2020csi}{{68}{2020}{{Tack et~al.}}{{Tack, Mo, Jeong, and Shin}}}
\bibcite{tishby2000information}{{69}{2000}{{Tishby et~al.}}{{Tishby, Pereira, and Bialek}}}
\bibcite{oord2018representation}{{70}{2018}{{van~den Oord et~al.}}{{van~den Oord, Li, and Vinyals}}}
\bibcite{vaswani2017attention}{{71}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2023clipn}{{72}{2023}{{Wang et~al.}}{{Wang, Li, Yao, and Li}}}
\bibcite{wang2021deep}{{73}{2021}{{Wang \& Deng}}{{Wang and Deng}}}
\bibcite{xiao2020likelihood}{{74}{2020}{{Xiao et~al.}}{{Xiao, Yan, and Amit}}}
\bibcite{xu2017information}{{75}{2017}{{Xu \& Raginsky}}{{Xu and Raginsky}}}
\bibcite{yangcan}{{76}{2025}{{Yang et~al.}}{{Yang, Yu, and Desell}}}
\bibcite{yang2021generalized}{{77}{2021}{{Yang et~al.}}{{Yang, Zhou, Li, and Liu}}}
\bibcite{yang2022openood}{{78}{2022}{{Yang et~al.}}{{Yang, Wang, Zou, Zhou, Ding, Peng, Wang, Chen, Li, Sun, et~al.}}}
\bibcite{zhang2023sirens}{{79}{2023{a}}{{Zhang et~al.}}{{Zhang, Li, Zhao, Xu, et~al.}}}
\bibcite{zhang2023openood}{{80}{2023{b}}{{Zhang et~al.}}{{Zhang, Yang, Wang, Wang, Lin, Zhang, Sun, Du, Li, Liu, et~al.}}}
\bibcite{zhang2021out}{{81}{2021}{{Zhang et~al.}}{{Zhang, Delbrouck, and Rubin}}}
\bibcite{zhou2022rethinking}{{82}{2022}{{Zhou}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{50}{}\protected@file@percent }
\@input{AppendixA.aux}
\gdef \@abspage@last{63}
