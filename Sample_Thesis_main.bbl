\begin{thebibliography}{122}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{AIPlanet}(2023)]{AIPlanet_DataSprint107_2024}
{AIPlanet}.
\newblock {Data Sprint 107 â€“ Butterfly Image Classification} [dataset].
\newblock
  \url{https://aiplanet.com/challenges/325/butterfly_identification/overview/about},
  2023.
\newblock Accessed: 2025-05-09.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{alemi2017deep}
Alexander~A Alemi, Ian Fischer, Joshua~V Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Bakator \& Radosav(2018)Bakator and Radosav]{bakator2018deep}
Mihalj Bakator and Dragica Radosav.
\newblock Deep learning and medical diagnosis: A review of literature.
\newblock \emph{Multimodal Technologies and Interaction}, 2\penalty0
  (3):\penalty0 47, 2018.

\bibitem[Bang et~al.(2025)Bang, Ji, Schelten, Hartshorn, Fowler, Zhang,
  Cancedda, and Fung]{bang2025hallulens}
Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng
  Zhang, Nicola Cancedda, and Pascale Fung.
\newblock Hallulens: Llm hallucination benchmark.
\newblock \emph{arXiv preprint arXiv:2504.17550}, 2025.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeshwar, Ozair, Bengio,
  Courville, and Hjelm]{belghazi2018mutual}
Mohamed~Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair,
  Yoshua Bengio, Aaron Courville, and Devon Hjelm.
\newblock Mutual information neural estimation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  531--540, 2018.

\bibitem[Berant et~al.(2013)Berant, Ceccaldi, Fader, Gabrilovich, Liang, and
  Zettlemoyer]{berant2013semantic}
Jonathan Berant, Andrew Ceccaldi, Antoine Fader, Evgeniy Gabrilovich, Percy
  Liang, and Luke Zettlemoyer.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pp.\  1533--1544, 2013.

\bibitem[Bossard et~al.(2014{\natexlab{a}})Bossard, Guillaumin, and
  Van~Gool]{bossard14}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101 -- mining discriminative components with random forests.
\newblock In \emph{European Conference on Computer Vision}, 2014{\natexlab{a}}.

\bibitem[Bossard et~al.(2014{\natexlab{b}})Bossard, Guillaumin, and
  Van~Gool]{food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101 -- mining discriminative components with random forests.
\newblock In \emph{European Conference on Computer Vision}, 2014{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and
  Steinhardt]{burns2023discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock \emph{arXiv preprint arXiv:2212.03827}, 2023.

\bibitem[Cao et~al.(2020)Cao, Huang, Hui, and Cohen]{cao2020benchmark}
Tianshi Cao, Chin-Wei Huang, David Yu-Tung Hui, and Joseph~Paul Cohen.
\newblock A benchmark of medical out of distribution detection.
\newblock \emph{arXiv preprint arXiv:2007.04250}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{International conference on machine learning}, pp.\
  1597--1607, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{chen2016infogan}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2172--2180, 2016.

\bibitem[Chern et~al.(2023)Chern, Chern, Chen, Qian, Wei, Zou, and
  Graham]{chern2023factool}
I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Qian, Kehua Wei, Chunting Zou,
  and Neubig Graham.
\newblock Factool: Factuality detection in generative ai--a tool augmented
  framework for multi-task and multi-domain scenarios.
\newblock \emph{arXiv preprint arXiv:2307.13528}, 2023.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and
  Vedaldi]{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3606--3613, 2014.

\bibitem[Cover \& Thomas(1999)Cover and Thomas]{cover1999elements}
Thomas~M Cover and Joy~A Thomas.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Drummond \& Shearer(2006)Drummond and Shearer]{drummond2006open}
Nick Drummond and Rob Shearer.
\newblock The open world assumption.
\newblock In \emph{eSI Workshop: The Closed World of Databases meets the Open
  World of the Semantic Web}, volume~15, pp.\ ~1, 2006.

\bibitem[Du et~al.(2024{\natexlab{a}})Du, Fang, Diakonikolas, and
  Li]{du2024does}
Xuefeng Du, Zhen Fang, Ilias Diakonikolas, and Yixuan Li.
\newblock How does unlabeled data provably help out-of-distribution detection?
\newblock \emph{arXiv preprint arXiv:2402.03502}, 2024{\natexlab{a}}.

\bibitem[Du et~al.(2024{\natexlab{b}})Du, Sun, and Li]{du2024and}
Xuefeng Du, Yiyou Sun, and Yixuan Li.
\newblock When and how does in-distribution label help out-of-distribution
  detection?
\newblock \emph{arXiv preprint arXiv:2405.18635}, 2024{\natexlab{b}}.

\bibitem[Ekim et~al.(2024)Ekim, Tadesse, Robinson, Hacheme, Schmitt, Dodhia,
  and Ferres]{ekim2024distribution}
Burak Ekim, Girmaw~Abebe Tadesse, Caleb Robinson, Gilles Hacheme, Michael
  Schmitt, Rahul Dodhia, and Juan M~Lavista Ferres.
\newblock Distribution shifts at scale: Out-of-distribution detection in earth
  observation.
\newblock \emph{arXiv preprint arXiv:2412.13394}, 2024.

\bibitem[Erhan et~al.(2013)Erhan, Goodfellow, Cukierski, and Bengio]{icmlface}
Dumitru Erhan, Ian Goodfellow, Will Cukierski, and Yoshua Bengio.
\newblock Challenges in representation learning: Facial expression recognition
  challenge, 2013.
\newblock URL
  \url{https://kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge}.

\bibitem[Esmaeilpour et~al.(2022)Esmaeilpour, Liu, Robertson, and
  Shu]{esmaeilpour2022zero}
Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu.
\newblock Zero-shot out-of-distribution detection based on the pre-trained
  model clip.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~36, pp.\  6568--6576, 2022.

\bibitem[Fang et~al.(2022)Fang, Li, Lu, Dong, Han, and Liu]{fang2022out}
Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo~Han, and Feng Liu.
\newblock Is out-of-distribution detection learnable?
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 37199--37213, 2022.

\bibitem[Farquhar et~al.(2024)Farquhar, Kossen, Kuhn, and
  Gal]{farquhar2024detecting}
Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal.
\newblock Detecting hallucinations in large language models using semantic
  entropy.
\newblock \emph{arXiv preprint arXiv:2406.15012}, 2024.

\bibitem[Federici et~al.(2020)Federici, Dutta, Forr{\'e}, Kushman, and
  Akata]{federici2020learning}
Marco Federici, Anjan Dutta, Patrick Forr{\'e}, Nate Kushman, and Zeynep Akata.
\newblock Learning robust representations via multi-view information
  bottleneck.
\newblock \emph{arXiv preprint arXiv:2002.07017}, 2020.

\bibitem[Fort et~al.(2021)Fort, Ren, and Lakshminarayanan]{fort2021exploring}
Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan.
\newblock Exploring the limits of out-of-distribution detection.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 7068--7081, 2021.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  6894--6910, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Guille-Escuret et~al.(2024)Guille-Escuret, Rodriguez, Vazquez,
  Mitliagkas, and Monteiro]{guille2024cadet}
Charles Guille-Escuret, Pau Rodriguez, David Vazquez, Ioannis Mitliagkas, and
  Joao Monteiro.
\newblock Cadet: Fully self-supervised out-of-distribution detection with
  contrastive learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  9729--9738, 2020.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and
  Borth]{helber2019eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing}, 2019.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mazeika, Kadavath, and
  Song]{hendrycks2019using}
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song.
\newblock Using self-supervised learning can improve model robustness and
  uncertainty.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Hewitt \& Manning(2019)Hewitt and Manning]{hewitt2019structural}
John Hewitt and Christopher~D Manning.
\newblock A structural probe for finding syntax in word representations.
\newblock \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4129--4138, 2019.

\bibitem[Hinton \& Salakhutdinov(2006)Hinton and
  Salakhutdinov]{hinton2006reducing}
Geoffrey~E Hinton and Ruslan~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{Science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2019learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Hossain et~al.(2021)Hossain, Uddin, Nahin, and Ibne~Eunus]{rock_data}
Shahriar Hossain, Jahir Uddin, Rakibul Nahin, and Salman Ibne~Eunus.
\newblock Rock classification dataset, 08 2021.

\bibitem[Hughes \& Salath{\'{e} }(2015)Hughes and Salath{\'{e} }]{plant}
David~P. Hughes and Marcel Salath{\'{e} }.
\newblock An open access repository of images on plant health to enable the
  development of mobile disease diagnostics through machine learning and
  crowdsourcing.
\newblock \emph{CoRR}, abs/1511.08060, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.08060}.

\bibitem[Hyv{\"a}rinen \& Oja(2000)Hyv{\"a}rinen and
  Oja]{hyvarinen2000independent}
Aapo Hyv{\"a}rinen and Erkki Oja.
\newblock Independent component analysis: algorithms and applications.
\newblock \emph{Neural networks}, 13\penalty0 (4-5):\penalty0 411--430, 2000.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan1999introduction}
Michael~I Jordan, Zoubin Ghahramani, Tommi~S Jaakkola, and Lawrence~K Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine Learning}, 37:\penalty0 183--233, 1999.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock \emph{Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pp.\  1601--1611, 2017.

\bibitem[Kafunah et~al.(2023)Kafunah, Verma, Ali, and Breslin]{kafunah2023out}
Jefkine Kafunah, Priyanka Verma, Muhammad~Intizar Ali, and John~G Breslin.
\newblock Out-of-distribution data generation for fault detection and diagnosis
  in industrial systems.
\newblock \emph{IEEE Access}, 11:\penalty0 135061--135073, 2023.

\bibitem[Katz-Samuels et~al.(2022)Katz-Samuels, Nakhleh, Nowak, and
  Li]{katz2022training}
Julian Katz-Samuels, Julia~B Nakhleh, Robert Nowak, and Yixuan Li.
\newblock Training ood detectors in their natural habitats.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10848--10865. PMLR, 2022.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 18661--18673, 2020.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas,
  et~al.]{kim2018interpretability}
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda
  Viegas, et~al.
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors (tcav).
\newblock \emph{International conference on machine learning}, pp.\
  2668--2677, 2018.

\bibitem[Kim et~al.(2021)Kim, Cho, and Lee]{kim2021wafer}
Yusung Kim, Donghee Cho, and Jee-Hyong Lee.
\newblock Wafer defect pattern classification with detecting
  out-of-distribution.
\newblock \emph{Microelectronics Reliability}, 122:\penalty0 114157, 2021.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2014auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and
  Fei-Fei]{KrauseStarkDengFei-Fei_3DRR2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)}, Sydney, Australia, 2013.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 and cifar-100 datasets, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee,
  et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 453--466, 2019.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2018)Lee, Lee, Lee, and Shin]{lee2018simple}
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2023)Li, Cheng, Zhao, Nie, and Wen]{li2023halueval}
Junyi Li, Xiaoxue Cheng, Wayne~Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
\newblock Halueval: A large-scale hallucination evaluation benchmark for large
  language models.
\newblock \emph{arXiv preprint arXiv:2305.11747}, 2023.

\bibitem[Liang et~al.(2017)Liang, Li, and Srikant]{liang2017enhancing}
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock \emph{arXiv preprint arXiv:1706.02690}, 2017.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  3214--3252, 2021.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics}, pp.\  3214--3252, 2022.

\bibitem[Linsker(1988)]{linsker1988self}
Ralph Linsker.
\newblock Self-organization in a perceptual network.
\newblock \emph{Computer}, 21\penalty0 (3):\penalty0 105--117, 1988.

\bibitem[Liu et~al.(2020)Liu, Wang, Owens, and Li]{liu2020energy}
Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.
\newblock Energy-based out-of-distribution detection.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21464--21475, 2020.

\bibitem[Liu et~al.(2023)Liu, Zhou, Wang, and Weinberger]{liu2023unsupervised}
Zhenzhen Liu, Jin~Peng Zhou, Yufan Wang, and Kilian~Q Weinberger.
\newblock Unsupervised out-of-distribution detection with diffusion inpainting.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22528--22538. PMLR, 2023.

\bibitem[Manakul et~al.(2023)Manakul, Liusie, and
  Gales]{manakul2023selfcheckgpt}
Potsawee Manakul, Adian Liusie, and Mark~JF Gales.
\newblock Selfcheckgpt: Zero-resource black-box hallucination detection for
  generative large language models.
\newblock \emph{arXiv preprint arXiv:2303.08896}, 2023.

\bibitem[McAllester(1999)]{mcallester1999pac}
David~A McAllester.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the 12th Annual Conference on Computational
  Learning Theory}, pp.\  164--170, 1999.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Narayanaswamy et~al.(2023)Narayanaswamy, Mubarka, Anirudh, Rajan, and
  Thiagarajan]{narayanaswamy2023exploring}
Vivek Narayanaswamy, Yamen Mubarka, Rushil Anirudh, Deepta Rajan, and
  Jayaraman~J Thiagarajan.
\newblock Exploring inlier and outlier specification for improved medical ood
  detection.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  4589--4598, 2023.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Olah et~al.(2020)Olah, Cammarata, Schubert, Goh, Petrov, and
  Carter]{olah2020zoom}
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and
  Shan Carter.
\newblock Zoom in: An introduction to circuits.
\newblock \emph{Distill}, 5\penalty0 (3):\penalty0 e00024--001, 2020.

\bibitem[Pearson(1901)]{pearson1901liii}
Karl Pearson.
\newblock Liii. on lines and planes of closest fit to systems of points in
  space.
\newblock \emph{The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 2\penalty0 (11):\penalty0 559--572, 1901.

\bibitem[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu,
  Chen, et~al.]{peng2023check}
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu~Hu, Qiuyuan
  Huang, Lars Liden, Zhou Yu, Weizhu Chen, et~al.
\newblock Check your facts and try again: Improving large language models with
  external knowledge and automated feedback.
\newblock \emph{arXiv preprint arXiv:2302.12813}, 2023.

\bibitem[Peng et~al.(2005)Peng, Long, and Ding]{peng2005feature}
Hanchuan Peng, Fuhui Long, and Chris Ding.
\newblock Feature selection based on mutual information criteria of
  max-dependency, max-relevance, and min-redundancy.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 27\penalty0 (8):\penalty0 1226--1238, 2005.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher~D Manning.
\newblock Glove: Global vectors for word representation.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pp.\  1532--1543, 2014.

\bibitem[Poole et~al.(2019)Poole, Ozair, Van Den~Oord, Alemi, and
  Tucker]{poole2019variational}
Ben Poole, Sherjil Ozair, Aaron Van Den~Oord, Alex Alemi, and George Tucker.
\newblock On variational bounds of mutual information.
\newblock \emph{International Conference on Machine Learning}, pp.\
  5171--5180, 2019.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training, 2018.
\newblock URL
  \url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu, et~al.]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, Peter~J Liu, et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Ramanagopal et~al.(2018)Ramanagopal, Anderson, Vasudevan, and
  Johnson-Roberson]{ramanagopal2018failing}
Manikandasriram~Srinivasan Ramanagopal, Cyrus Anderson, Ram Vasudevan, and
  Matthew Johnson-Roberson.
\newblock Failing to learn: Autonomously identifying perception failures for
  self-driving cars.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (4):\penalty0
  3860--3867, 2018.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International conference on machine learning}, pp.\
  1278--1286. PMLR, 2014.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers2020primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer on neural network models for natural language processing.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  65--95, 2020.

\bibitem[Runge et~al.(2019)Runge, Nowack, Kretschmer, Flaxman, and
  Sejdinovic]{runge2019detecting}
Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino
  Sejdinovic.
\newblock Detecting and quantifying causal associations in large nonlinear time
  series datasets.
\newblock \emph{Science Advances}, 5\penalty0 (11):\penalty0 eaau4996, 2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Saadati et~al.(2024)Saadati, Balu, Chiranjeevi, Jubery, Singh, Sarkar,
  Singh, and Ganapathysubramanian]{saadati2024out}
Mojdeh Saadati, Aditya Balu, Shivani Chiranjeevi, Talukder~Zaki Jubery,
  Asheesh~K Singh, Soumik Sarkar, Arti Singh, and Baskar Ganapathysubramanian.
\newblock Out-of-distribution detection algorithms for robust insect
  classification.
\newblock \emph{Plant Phenomics}, 6:\penalty0 0170, 2024.

\bibitem[Saxe et~al.(2019)Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey,
  and Cox]{saxe2019information}
Andrew~M Saxe, Yamini Bansal, Joel Dapello, Madhu~S Advani, Artemy Kolchinsky,
  Brendan~D Tracey, and David~D Cox.
\newblock The information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124020, 2019.

\bibitem[Sehwag et~al.(2021)Sehwag, Chiang, and Mittal]{sehwag2021ssd}
Vikash Sehwag, Mung Chiang, and Prateek Mittal.
\newblock Ssd: A unified framework for self-supervised outlier detection.
\newblock \emph{arXiv preprint arXiv:2103.12051}, 2021.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  618--626, 2017.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Claude~Elwood Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3):\penalty0
  379--423, 1948.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  2556--2565,
  2018.

\bibitem[Shwartz-Ziv \& LeCun(2023)Shwartz-Ziv and LeCun]{shwartz2023compress}
Ravid Shwartz-Ziv and Yann LeCun.
\newblock To compress or not to compress--self-supervised learning and
  information theory: A review.
\newblock \emph{arXiv preprint arXiv:2304.09355}, 2023.

\bibitem[Shwartz-Ziv \& Tishby(2017)Shwartz-Ziv and Tishby]{shwartz2017opening}
Ravid Shwartz-Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Single et~al.(2023)Single, Jain, and Jain]{single2023realwaste}
Nikita Single, Harsh Jain, and Priyanka Jain.
\newblock Realwaste: A novel real-life data set for landfill waste
  classification using deep learning.
\newblock \emph{IEEE Access}, 11:\penalty0 112562--112584, 2023.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Soni(2020)]{card_data}
Aditya Soni.
\newblock Playing cards dataset, 2020.
\newblock URL
  \url{https://www.kaggle.com/datasets/adityasoni04/playing-cards-dataset}.

\bibitem[sumanthvrao(2020)]{yoga_data}
sumanthvrao.
\newblock Yoga poses, 2020.
\newblock URL \url{https://www.kaggle.com/datasets/sumanthvrao/yoga-poses}.
\newblock Version 6.

\bibitem[Sun et~al.(2022)Sun, Ming, Zhu, and Li]{sun2022out}
Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li.
\newblock Out-of-distribution detection with deep nearest neighbors.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  20827--20840. PMLR, 2022.

\bibitem[Tack et~al.(2020)Tack, Mo, Jeong, and Shin]{tack2020csi}
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin.
\newblock Csi: Novelty detection via contrastive learning on distributionally
  shifted instances.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 11839--11852, 2020.

\bibitem[Tenney et~al.(2019)Tenney, Das, and Pavlick]{tenney2019bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock Bert rediscovers the classical nlp pipeline.
\newblock \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pp.\  4593--4601, 2019.

\bibitem[Thorne et~al.(2018)Thorne, Vlachos, Christodoulopoulos, and
  Mittal]{thorne2018fever}
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
\newblock Fever: a large-scale dataset for fact extraction and verification.
\newblock \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pp.\  809--819, 2018.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Naftali Tishby, Fernando~C Pereira, and William Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and
  Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock In \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita \& Titov(2020)Voita and Titov]{voita2019information}
Elena Voita and Ivan Titov.
\newblock Information-theoretic probing with minimum description length.
\newblock \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  183--196, 2020.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pp.\  5797--5808, 2019.

\bibitem[Wang et~al.(2023)Wang, Li, Yao, and Li]{wang2023clipn}
Hualiang Wang, Yi~Li, Huifeng Yao, and Xiaomeng Li.
\newblock Clipn for zero-shot ood detection: Teaching clip to say no.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1802--1812, 2023.

\bibitem[Wang \& Deng(2021)Wang and Deng]{wang2021deep}
Mei Wang and Weihong Deng.
\newblock Deep face recognition: A survey.
\newblock \emph{Neurocomputing}, 429:\penalty0 215--244, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{CoRR}, abs/1708.07747, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.07747}.

\bibitem[Xiao et~al.(2020)Xiao, Yan, and Amit]{xiao2020likelihood}
Zhisheng Xiao, Qing Yan, and Yali Amit.
\newblock Likelihood regret: An out-of-distribution detection score for
  variational auto-encoder.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20685--20696, 2020.

\bibitem[Xu \& Raginsky(2017)Xu and Raginsky]{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0
  (9):\penalty0 5948--5964, 2017.

\bibitem[Xu et~al.(2024)Xu, Yu, Xu, Inkawhich, and Chen]{pmlr-v235-xu24ae}
Chenhui Xu, Fuxun Yu, Zirui Xu, Nathan Inkawhich, and Xiang Chen.
\newblock Out-of-distribution detection via deep multi-comprehension ensemble.
\newblock In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian
  Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.),
  \emph{Proceedings of the 41st International Conference on Machine Learning},
  volume 235 of \emph{Proceedings of Machine Learning Research}, pp.\
  55465--55489. PMLR, 21--27 Jul 2024.
\newblock URL \url{https://proceedings.mlr.press/v235/xu24ae.html}.

\bibitem[Yang et~al.(2025)Yang, Yu, and Desell]{yangcan}
Hong Yang, Qi~Yu, and Travis Desell.
\newblock Can we ignore labels in out of distribution detection?
\newblock In \emph{The Thirteenth International Conference on Learning
  Representations}, 2025.

\bibitem[Yang et~al.(2023)Yang, Shi, Wei, Liu, Zhao, Ke, Pfister, and
  Ni]{yang2023medmnist}
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,
  Hanspeter Pfister, and Bingbing Ni.
\newblock Medmnist v2-a large-scale lightweight benchmark for 2d and 3d
  biomedical image classification.
\newblock \emph{Scientific Data}, 10\penalty0 (1):\penalty0 41, 2023.

\bibitem[Yang et~al.(2021)Yang, Zhou, Li, and Liu]{yang2021generalized}
Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu.
\newblock Generalized out-of-distribution detection: A survey.
\newblock \emph{arXiv preprint arXiv:2110.11334}, 2021.

\bibitem[Yang et~al.(2022)Yang, Wang, Zou, Zhou, Ding, Peng, Wang, Chen, Li,
  Sun, et~al.]{yang2022openood}
Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan
  Peng, Haoqi Wang, Guangyao Chen, Bo~Li, Yiyou Sun, et~al.
\newblock Openood: Benchmarking generalized out-of-distribution detection.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 32598--32611, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Li, Zhao, Xu,
  et~al.]{zhang2023sirens}
Hanlin Zhang, Ziyang Li, Yuxin Zhao, Sheng Xu, et~al.
\newblock Sirens: Detecting hallucinations in large language models using
  uncertainty.
\newblock \emph{arXiv preprint arXiv:2310.13988}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Yang, Wang, Wang, Lin, Zhang,
  Sun, Du, Li, Liu, et~al.]{zhang2023openood}
Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran
  Zhang, Yiyou Sun, Xuefeng Du, Yixuan Li, Ziwei Liu, et~al.
\newblock Openood v1. 5: Enhanced benchmark for out-of-distribution detection.
\newblock \emph{arXiv preprint arXiv:2306.09301}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Delbrouck, and Rubin]{zhang2021out}
Oliver Zhang, Jean-Benoit Delbrouck, and Daniel~L Rubin.
\newblock Out of distribution detection for medical images.
\newblock In \emph{Uncertainty for Safe Utilization of Machine Learning in
  Medical Imaging, and Perinatal Imaging, Placental and Preterm Image Analysis:
  3rd International Workshop, UNSURE 2021, and 6th International Workshop,
  PIPPI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October
  1, 2021, Proceedings 3}, pp.\  102--111. Springer, 2021.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 40\penalty0 (6):\penalty0 1452--1464, 2017.

\bibitem[Zhou(2022)]{zhou2022rethinking}
Yibo Zhou.
\newblock Rethinking reconstruction autoencoder-based out-of-distribution
  detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7379--7387, 2022.

\end{thebibliography}
