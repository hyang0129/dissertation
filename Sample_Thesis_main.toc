\contentsline {chapter}{\numberline {1}Introduction}{1}{}%
\contentsline {chapter}{\numberline {2}Background and Definitions}{2}{}%
\contentsline {section}{\numberline {2.1}Out of Distribution Detection}{2}{}%
\contentsline {section}{\numberline {2.2}Anomaly Detection}{3}{}%
\contentsline {subsection}{\numberline {2.2.1}Definiton and Scope}{4}{}%
\contentsline {subsection}{\numberline {2.2.2}Training Assumptions}{4}{}%
\contentsline {subsection}{\numberline {2.2.3}Evaluation Settings}{4}{}%
\contentsline {section}{\numberline {2.3}Information Theory}{5}{}%
\contentsline {subsection}{\numberline {2.3.1}Entropy and Mutual Information}{6}{}%
\contentsline {section}{\numberline {2.4}Information Bottleneck and Minimal Sufficient Statistic}{6}{}%
\contentsline {subsection}{\numberline {2.4.1}Minimal Sufficient Statistic}{7}{}%
\contentsline {subsection}{\numberline {2.4.2}Information Bottleneck}{7}{}%
\contentsline {section}{\numberline {2.5}Dataset Domain}{8}{}%
\contentsline {subsection}{\numberline {2.5.1}Domain Features and Domain Feature Collapse}{10}{}%
\contentsline {section}{\numberline {2.6}Unlabeled OOD Detection}{11}{}%
\contentsline {section}{\numberline {2.7}Large Language Models}{12}{}%
\contentsline {chapter}{\numberline {3}Literature Review}{15}{}%
\contentsline {section}{\numberline {3.1}Information Theory in Machine Learning}{15}{}%
\contentsline {section}{\numberline {3.2}Representation Learning}{16}{}%
\contentsline {subsection}{\numberline {3.2.1}Unsupervised Representation Learning}{17}{}%
\contentsline {subsection}{\numberline {3.2.2}Self-Supervised Learning}{17}{}%
\contentsline {section}{\numberline {3.3}Out of Distribution Detection}{18}{}%
\contentsline {subsection}{\numberline {3.3.1}Classical and Training-Agnostic Approaches}{18}{}%
\contentsline {subsection}{\numberline {3.3.2}Self-Supervised and Unsupervised OOD Detection}{19}{}%
\contentsline {subsection}{\numberline {3.3.3}Benchmarking and Evaluation}{19}{}%
\contentsline {subsection}{\numberline {3.3.4}Single-Domain OOD Detection}{20}{}%
\contentsline {subsection}{\numberline {3.3.5}Domain Adaptation and Transfer Learning}{21}{}%
\contentsline {subsection}{\numberline {3.3.6}Multi-Stage and Ensemble Approaches}{22}{}%
\contentsline {section}{\numberline {3.4}Hallucination Detection}{22}{}%
\contentsline {subsection}{\numberline {3.4.1}Taxonomy of Hallucination Detection Approaches}{23}{}%
\contentsline {subsection}{\numberline {3.4.2}Information-Theoretic Perspectives}{23}{}%
\contentsline {subsection}{\numberline {3.4.3}Evaluation and Benchmarks}{24}{}%
\contentsline {section}{\numberline {3.5}Model Architectures}{24}{}%
\contentsline {subsection}{\numberline {3.5.1}Convolutional Neural Networks}{24}{}%
\contentsline {subsection}{\numberline {3.5.2}Transformers}{24}{}%
\contentsline {subsection}{\numberline {3.5.3}Foundation Models}{25}{}%
\contentsline {chapter}{\numberline {4}Label Blindness in Unlabeled OOD Detection}{26}{}%
\contentsline {section}{\numberline {4.1}Introduction}{26}{}%
\contentsline {section}{\numberline {4.2}Preliminaries}{29}{}%
\contentsline {subsection}{\numberline {4.2.1}Labeled and Unlabeled Out-of-Distribution Detection}{29}{}%
\contentsline {subsection}{\numberline {4.2.2}Self-Supervised and Unsupervised Learning}{29}{}%
\contentsline {section}{\numberline {4.3}Guaranteed OOD Detection Failure}{31}{}%
\contentsline {subsection}{\numberline {4.3.1}Label Blindness Theorem (Strict Label Blindness)}{31}{}%
\contentsline {subsection}{\numberline {4.3.2}Implications of Strict Label Blindness in Real World Situations}{33}{}%
\contentsline {subsection}{\numberline {4.3.3}Theoretical Implications}{33}{}%
\contentsline {section}{\numberline {4.4}Benchmarking for Label Blindness Failure}{34}{}%
\contentsline {subsection}{\numberline {4.4.1}Bootstrapping and the Adjacent OOD Benchmark}{34}{}%
\contentsline {subsection}{\numberline {4.4.2}Why Adjacent OOD is Safety-Critical to Almost All Real World Systems}{34}{}%
\contentsline {subsection}{\numberline {4.4.3}Comparing Adjacent, Near, and Far OOD Benchmarks}{35}{}%
\contentsline {subsection}{\numberline {4.4.4}Implications for OOD from Unlabeled Data}{35}{}%
\contentsline {section}{\numberline {4.5}Experimental Results}{36}{}%
\contentsline {subsection}{\numberline {4.5.1}Experimental Setup}{36}{}%
\contentsline {paragraph}{Supervised Baseline.}{36}{}%
\contentsline {paragraph}{Self-supervised Baselines.}{36}{}%
\contentsline {paragraph}{Unsupervised Baseline.}{37}{}%
\contentsline {paragraph}{Zero-shot Baseline.}{37}{}%
\contentsline {subsection}{\numberline {4.5.2}Adjacent OOD Datasets}{37}{}%
\contentsline {subsection}{\numberline {4.5.3}Experimental Results}{38}{}%
\contentsline {section}{\numberline {4.6}Discussion}{39}{}%
\contentsline {subsection}{\numberline {4.6.1}Impact of Label Blindness on Future Research}{39}{}%
\contentsline {subsection}{\numberline {4.6.2}Recommendations for Future OOD Detection Research}{40}{}%
\contentsline {section}{\numberline {4.7}Conclusion}{40}{}%
\contentsline {chapter}{\numberline {5}Domain Feature Collapse in Single-Domain OOD Detection}{42}{}%
\contentsline {section}{\numberline {5.1}Introduction}{42}{}%
\contentsline {section}{\numberline {5.2}Problem Formulation}{43}{}%
\contentsline {subsection}{\numberline {5.2.1}Single-Domain Datasets and Domain Features}{43}{}%
\contentsline {section}{\numberline {5.3}Theoretical Analysis: Domain Feature Collapse}{44}{}%
\contentsline {subsection}{\numberline {5.3.1}Implications for OOD Detection}{45}{}%
\contentsline {section}{\numberline {5.4}Domain Filtering: A Solution to Domain Feature Collapse}{46}{}%
\contentsline {subsection}{\numberline {5.4.1}Two-Stage Detector: Domain Filtering + OOD Detection}{46}{}%
\contentsline {subsection}{\numberline {5.4.2}Relationship to Near, Far, and Adjacent OOD}{47}{}%
\contentsline {section}{\numberline {5.5}Experimental Validation}{47}{}%
\contentsline {subsection}{\numberline {5.5.1}Domain Bench: Single-Domain Datasets}{47}{}%
\contentsline {subsection}{\numberline {5.5.2}Experimental Setup}{48}{}%
\contentsline {subsection}{\numberline {5.5.3}Results and Analysis}{49}{}%
\contentsline {subsection}{\numberline {5.5.4}Detailed Results by Dataset}{50}{}%
\contentsline {subsection}{\numberline {5.5.5}Case Study: Colon Dataset}{51}{}%
\contentsline {subsection}{\numberline {5.5.6}Discussion}{52}{}%
\contentsline {subsubsection}{Rock Dataset as an Outlier}{52}{}%
\contentsline {section}{\numberline {5.6}Limitations and Future Work}{53}{}%
\contentsline {subsection}{\numberline {5.6.1}Assumptions and Scope}{53}{}%
\contentsline {subsection}{\numberline {5.6.2}Generalization to Other Domains}{53}{}%
\contentsline {subsection}{\numberline {5.6.3}Alternative Solutions}{53}{}%
\contentsline {section}{\numberline {5.7}Conclusion}{54}{}%
\contentsline {chapter}{\numberline {6}Hallucinations Through the Lens of Mutual Information and Representation Learning}{56}{}%
\contentsline {section}{\numberline {6.1}Introduction and Motivation}{56}{}%
\contentsline {section}{\numberline {6.2}Theoretical Framework}{57}{}%
\contentsline {subsection}{\numberline {6.2.1}Mutual Information in Language Generation}{57}{}%
\contentsline {subsection}{\numberline {6.2.2}Hallucination as Information Loss}{57}{}%
\contentsline {section}{\numberline {6.3}Proposed Research Methodology}{58}{}%
\contentsline {subsection}{\numberline {6.3.1}Mutual Information Estimation in Foundation Models}{58}{}%
\contentsline {subsubsection}{Neural Mutual Information Estimation}{58}{}%
\contentsline {subsubsection}{Variational Bounds}{59}{}%
\contentsline {subsubsection}{Kernel-Based Methods}{59}{}%
\contentsline {subsubsection}{Discrete Approximations}{60}{}%
\contentsline {subsubsection}{Contrastive Mutual Information Estimation (Proposed)}{61}{}%
\contentsline {paragraph}{Method Overview and Motivation}{61}{}%
\contentsline {paragraph}{Formal Mathematical Framework}{61}{}%
\contentsline {paragraph}{Advantages and Theoretical Justification}{62}{}%
\contentsline {paragraph}{Limitations and Challenges}{63}{}%
\contentsline {paragraph}{Implementation Considerations}{63}{}%
\contentsline {subsection}{\numberline {6.3.2}Representation Learning Analysis}{64}{}%
\contentsline {subsubsection}{Layer-wise Information Flow}{64}{}%
\contentsline {subsubsection}{Attention Mechanism Analysis}{64}{}%
\contentsline {subsubsection}{Information Bottleneck Dynamics}{65}{}%
\contentsline {section}{\numberline {6.4}Experimental Design}{65}{}%
\contentsline {subsection}{\numberline {6.4.1}Datasets and Benchmarks}{65}{}%
\contentsline {subsubsection}{Factual Question Answering Datasets}{65}{}%
\contentsline {paragraph}{Natural Questions}{65}{}%
\contentsline {paragraph}{TriviaQA}{66}{}%
\contentsline {paragraph}{WebQuestions}{66}{}%
\contentsline {subsubsection}{Hallucination-Specific Benchmarks}{66}{}%
\contentsline {paragraph}{HaluEval}{66}{}%
\contentsline {paragraph}{TruthfulQA}{67}{}%
\contentsline {paragraph}{FEVER}{67}{}%
\contentsline {subsubsection}{Synthetic Validation Datasets}{68}{}%
\contentsline {paragraph}{Gaussian Mixture Models}{68}{}%
\contentsline {paragraph}{Transformer-Based Synthetic Data}{68}{}%
\contentsline {subsection}{\numberline {6.4.2}Model Analysis}{68}{}%
\contentsline {subsubsection}{Architecture Comparison}{68}{}%
\contentsline {paragraph}{Transformer-Based Models}{68}{}%
\contentsline {paragraph}{State-Space Models}{69}{}%
\contentsline {paragraph}{Hybrid Architectures}{69}{}%
\contentsline {subsubsection}{Scale Analysis}{69}{}%
\contentsline {paragraph}{Parameter Count Effects}{69}{}%
\contentsline {paragraph}{Training Data Scale}{70}{}%
\contentsline {paragraph}{Context Length Analysis}{70}{}%
\contentsline {subsection}{\numberline {6.4.3}Evaluation Metrics and Protocols}{70}{}%
\contentsline {subsubsection}{Mutual Information Estimation Metrics}{70}{}%
\contentsline {paragraph}{Synthetic Data Validation}{70}{}%
\contentsline {paragraph}{Cross-Method Consistency}{71}{}%
\contentsline {subsubsection}{Hallucination Detection Metrics}{71}{}%
\contentsline {paragraph}{Area Under the Receiver Operating Characteristic Curve (AUROC)}{71}{}%
\contentsline {paragraph}{False Positive Rate at 95\% True Positive Rate (FPR95)}{71}{}%
\contentsline {paragraph}{Precision-Recall Analysis}{71}{}%
\contentsline {subsubsection}{Statistical Significance and Robustness}{72}{}%
\contentsline {paragraph}{Cross-Validation and Bootstrap Sampling}{72}{}%
\contentsline {paragraph}{Multiple Random Seeds}{72}{}%
\contentsline {paragraph}{Ablation Studies}{72}{}%
\contentsline {subsubsection}{Contrastive MI Estimation Validation}{72}{}%
\contentsline {paragraph}{Synthetic Validation Protocol}{72}{}%
\contentsline {paragraph}{Cross-Method Comparison Framework}{73}{}%
\contentsline {paragraph}{Layer Consistency Analysis}{73}{}%
\contentsline {paragraph}{Comprehensive Ablation Studies}{74}{}%
\contentsline {paragraph}{Computational Efficiency Benchmarking}{74}{}%
\contentsline {paragraph}{Hallucination Correlation Validation}{75}{}%
\contentsline {section}{\numberline {6.5}Expected Contributions}{75}{}%
\contentsline {subsection}{\numberline {6.5.1}Theoretical Contributions}{75}{}%
\contentsline {subsection}{\numberline {6.5.2}Empirical Contributions}{76}{}%
\contentsline {subsection}{\numberline {6.5.3}Practical Applications}{76}{}%
\contentsline {section}{\numberline {6.6}Challenges and Limitations}{76}{}%
\contentsline {subsection}{\numberline {6.6.1}Technical Challenges}{76}{}%
\contentsline {subsection}{\numberline {6.6.2}Theoretical Limitations}{76}{}%
\contentsline {section}{\numberline {6.7}Related Work and Positioning}{77}{}%
\contentsline {subsection}{\numberline {6.7.1}Information Theory in Natural Language Processing}{77}{}%
\contentsline {subsubsection}{Classical Information-Theoretic Approaches}{77}{}%
\contentsline {subsubsection}{Mutual Information in Representation Learning}{77}{}%
\contentsline {subsubsection}{Information Bottleneck Theory}{77}{}%
\contentsline {subsection}{\numberline {6.7.2}Hallucination Detection and Mitigation}{78}{}%
\contentsline {subsubsection}{Confidence-Based Methods}{78}{}%
\contentsline {subsubsection}{Consistency-Based Approaches}{78}{}%
\contentsline {subsubsection}{Mechanistic Approaches}{79}{}%
\contentsline {subsection}{\numberline {6.7.3}Representation Learning in Language Models}{79}{}%
\contentsline {subsubsection}{Probing Studies}{79}{}%
\contentsline {subsubsection}{Mechanistic Interpretability}{79}{}%
\contentsline {subsubsection}{Information Flow Analysis}{80}{}%
\contentsline {subsection}{\numberline {6.7.4}Contrastive Learning in NLP}{80}{}%
\contentsline {subsubsection}{Sentence and Document Representations}{80}{}%
\contentsline {subsubsection}{Mutual Information Estimation via Contrastive Learning}{80}{}%
\contentsline {subsection}{\numberline {6.7.5}Positioning and Novel Contributions}{81}{}%
\contentsline {subsubsection}{Theoretical Contributions}{81}{}%
\contentsline {subsubsection}{Methodological Innovations}{81}{}%
\contentsline {subsubsection}{Bridging Theory and Practice}{81}{}%
\contentsline {subsubsection}{Relationship to Existing Work}{82}{}%
\contentsline {section}{\numberline {6.8}Conclusion}{82}{}%
\contentsline {chapter}{\numberline {7}Research Timeline}{83}{}%
\contentsline {section}{\numberline {7.1}Overview}{83}{}%
\contentsline {section}{\numberline {7.2}Phase 1: Foundation and Method Development (Months 1-4)}{83}{}%
\contentsline {subsection}{\numberline {7.2.1}Month 1: Theoretical Framework}{83}{}%
\contentsline {subsection}{\numberline {7.2.2}Month 2: Contrastive MI Implementation}{84}{}%
\contentsline {subsection}{\numberline {7.2.3}Month 3: Baseline Methods and Comparison}{84}{}%
\contentsline {subsection}{\numberline {7.2.4}Month 4: Method Refinement}{84}{}%
\contentsline {section}{\numberline {7.3}Phase 2: Large-Scale Validation and Hallucination Detection (Months 5-8)}{84}{}%
\contentsline {subsection}{\numberline {7.3.1}Month 5: Foundation Model Analysis}{84}{}%
\contentsline {subsection}{\numberline {7.3.2}Month 6: Hallucination Correlation Studies}{85}{}%
\contentsline {subsection}{\numberline {7.3.3}Month 7: Detection System Development}{85}{}%
\contentsline {subsection}{\numberline {7.3.4}Month 8: Cross-Architecture Validation}{85}{}%
\contentsline {section}{\numberline {7.4}Phase 3: Applications and Deployment (Months 9-12)}{85}{}%
\contentsline {subsection}{\numberline {7.4.1}Month 9: Domain-Specific Applications}{85}{}%
\contentsline {subsection}{\numberline {7.4.2}Month 10: Intervention Strategies}{86}{}%
\contentsline {subsection}{\numberline {7.4.3}Month 11: Comprehensive Evaluation}{86}{}%
\contentsline {subsection}{\numberline {7.4.4}Month 12: Documentation and Dissemination}{86}{}%
\contentsline {section}{\numberline {7.5}Key Deliverables}{86}{}%
\contentsline {subsection}{\numberline {7.5.1}Technical Deliverables}{86}{}%
\contentsline {subsection}{\numberline {7.5.2}Research Outputs}{87}{}%
\contentsline {section}{\numberline {7.6}Risk Mitigation}{87}{}%
\contentsline {subsection}{\numberline {7.6.1}Technical Risks}{87}{}%
\contentsline {subsection}{\numberline {7.6.2}Timeline Risks}{87}{}%
\contentsline {section}{\numberline {7.7}Success Metrics}{87}{}%
\contentsline {subsection}{\numberline {7.7.1}Quantitative Metrics}{87}{}%
\contentsline {subsection}{\numberline {7.7.2}Qualitative Metrics}{88}{}%
\contentsline {chapter}{\numberline {8}Discussion}{89}{}%
\contentsline {chapter}{Appendices}{102}{}%
\contentsline {chapter}{\numberline {A}Theoretical Proofs for Label Blindness}{103}{}%
\contentsline {section}{\numberline {A.1}Properties of Mutual Information and Entropy}{103}{}%
\contentsline {section}{\numberline {A.2}Supporting Theorems and Proofs}{104}{}%
\contentsline {subsection}{\numberline {A.2.1}Sufficiency}{104}{}%
\contentsline {subsection}{\numberline {A.2.2}Lower Bound of Mutual Information for Sufficiency}{105}{}%
\contentsline {subsection}{\numberline {A.2.3}Conditional Mutual Information of Noise}{106}{}%
\contentsline {subsection}{\numberline {A.2.4}Factorization of Bottleneck Loss}{107}{}%
\contentsline {section}{\numberline {A.3}Main Theorems and Proofs}{108}{}%
\contentsline {subsection}{\numberline {A.3.1}Strict Label Blindness in the Minimal Sufficient Statistic}{108}{}%
\contentsline {subsection}{\numberline {A.3.2}Independence of Filtered Distributions}{113}{}%
\contentsline {subsection}{\numberline {A.3.3}Strict Label Blindness in Filtered Distributions - Guaranteed OOD Failure}{114}{}%
\contentsline {subsection}{\numberline {A.3.4}Unavoidable Risk of Overlapping Out of Distribution Data}{116}{}%
