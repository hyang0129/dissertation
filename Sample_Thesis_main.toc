\contentsline {chapter}{\numberline {1}Introduction}{1}{}%
\contentsline {chapter}{\numberline {2}Background and Definitions}{2}{}%
\contentsline {section}{\numberline {2.1}Out of Distribution Detection}{2}{}%
\contentsline {section}{\numberline {2.2}Anomaly Detection}{3}{}%
\contentsline {subsection}{\numberline {2.2.1}Definiton and Scope}{4}{}%
\contentsline {subsection}{\numberline {2.2.2}Training Assumptions}{4}{}%
\contentsline {subsection}{\numberline {2.2.3}Evaluation Settings}{4}{}%
\contentsline {section}{\numberline {2.3}Information Theory}{5}{}%
\contentsline {subsection}{\numberline {2.3.1}Entropy and Mutual Information}{6}{}%
\contentsline {section}{\numberline {2.4}Information Bottleneck and Minimal Sufficient Statistic}{6}{}%
\contentsline {subsection}{\numberline {2.4.1}Minimal Sufficient Statistic}{7}{}%
\contentsline {subsection}{\numberline {2.4.2}Information Bottleneck}{7}{}%
\contentsline {section}{\numberline {2.5}Dataset Domain}{8}{}%
\contentsline {subsection}{\numberline {2.5.1}Domain Features and Domain Feature Collapse}{10}{}%
\contentsline {section}{\numberline {2.6}Unlabeled OOD Detection}{11}{}%
\contentsline {section}{\numberline {2.7}Large Language Models}{12}{}%
\contentsline {chapter}{\numberline {3}Literature Review}{15}{}%
\contentsline {section}{\numberline {3.1}Information Theory in Machine Learning}{15}{}%
\contentsline {section}{\numberline {3.2}Representation Learning}{16}{}%
\contentsline {subsection}{\numberline {3.2.1}Unsupervised Representation Learning}{17}{}%
\contentsline {subsection}{\numberline {3.2.2}Self-Supervised Learning}{17}{}%
\contentsline {section}{\numberline {3.3}Out of Distribution Detection}{18}{}%
\contentsline {subsection}{\numberline {3.3.1}Classical and Training-Agnostic Approaches}{18}{}%
\contentsline {subsection}{\numberline {3.3.2}Self-Supervised and Unsupervised OOD Detection}{19}{}%
\contentsline {subsection}{\numberline {3.3.3}Benchmarking and Evaluation}{19}{}%
\contentsline {subsection}{\numberline {3.3.4}Single-Domain OOD Detection}{20}{}%
\contentsline {subsection}{\numberline {3.3.5}Domain Adaptation and Transfer Learning}{21}{}%
\contentsline {subsection}{\numberline {3.3.6}Multi-Stage and Ensemble Approaches}{22}{}%
\contentsline {section}{\numberline {3.4}Hallucination Detection}{22}{}%
\contentsline {subsection}{\numberline {3.4.1}Taxonomy of Hallucination Detection Approaches}{23}{}%
\contentsline {subsection}{\numberline {3.4.2}Information-Theoretic Perspectives}{23}{}%
\contentsline {subsection}{\numberline {3.4.3}Evaluation and Benchmarks}{24}{}%
\contentsline {section}{\numberline {3.5}Model Architectures}{24}{}%
\contentsline {subsection}{\numberline {3.5.1}Convolutional Neural Networks}{24}{}%
\contentsline {subsection}{\numberline {3.5.2}Transformers}{24}{}%
\contentsline {subsection}{\numberline {3.5.3}Foundation Models}{25}{}%
\contentsline {chapter}{\numberline {4}Label Blindness in Unlabeled OOD Detection}{26}{}%
\contentsline {section}{\numberline {4.1}Introduction}{26}{}%
\contentsline {section}{\numberline {4.2}Preliminaries}{29}{}%
\contentsline {subsection}{\numberline {4.2.1}Labeled and Unlabeled Out-of-Distribution Detection}{29}{}%
\contentsline {subsection}{\numberline {4.2.2}Self-Supervised and Unsupervised Learning}{29}{}%
\contentsline {section}{\numberline {4.3}Guaranteed OOD Detection Failure}{31}{}%
\contentsline {subsection}{\numberline {4.3.1}Label Blindness Theorem (Strict Label Blindness)}{31}{}%
\contentsline {subsection}{\numberline {4.3.2}Implications of Strict Label Blindness in Real World Situations}{33}{}%
\contentsline {subsection}{\numberline {4.3.3}Theoretical Implications}{33}{}%
\contentsline {section}{\numberline {4.4}Benchmarking for Label Blindness Failure}{34}{}%
\contentsline {subsection}{\numberline {4.4.1}Bootstrapping and the Adjacent OOD Benchmark}{34}{}%
\contentsline {subsection}{\numberline {4.4.2}Why Adjacent OOD is Safety-Critical to Almost All Real World Systems}{34}{}%
\contentsline {subsection}{\numberline {4.4.3}Comparing Adjacent, Near, and Far OOD Benchmarks}{35}{}%
\contentsline {subsection}{\numberline {4.4.4}Implications for OOD from Unlabeled Data}{35}{}%
\contentsline {section}{\numberline {4.5}Experimental Results}{36}{}%
\contentsline {subsection}{\numberline {4.5.1}Experimental Setup}{36}{}%
\contentsline {paragraph}{Supervised Baseline.}{36}{}%
\contentsline {paragraph}{Self-supervised Baselines.}{36}{}%
\contentsline {paragraph}{Unsupervised Baseline.}{37}{}%
\contentsline {paragraph}{Zero-shot Baseline.}{37}{}%
\contentsline {subsection}{\numberline {4.5.2}Adjacent OOD Datasets}{37}{}%
\contentsline {subsection}{\numberline {4.5.3}Experimental Results}{38}{}%
\contentsline {section}{\numberline {4.6}Discussion}{39}{}%
\contentsline {subsection}{\numberline {4.6.1}Impact of Label Blindness on Future Research}{39}{}%
\contentsline {subsection}{\numberline {4.6.2}Recommendations for Future OOD Detection Research}{40}{}%
\contentsline {section}{\numberline {4.7}Conclusion}{40}{}%
\contentsline {chapter}{\numberline {5}Domain Feature Collapse in Single-Domain OOD Detection}{42}{}%
\contentsline {section}{\numberline {5.1}Introduction}{42}{}%
\contentsline {section}{\numberline {5.2}Problem Formulation}{43}{}%
\contentsline {subsection}{\numberline {5.2.1}Single-Domain Datasets and Domain Features}{43}{}%
\contentsline {section}{\numberline {5.3}Theoretical Analysis: Domain Feature Collapse}{44}{}%
\contentsline {subsection}{\numberline {5.3.1}Implications for OOD Detection}{45}{}%
\contentsline {section}{\numberline {5.4}Domain Filtering: A Solution to Domain Feature Collapse}{46}{}%
\contentsline {subsection}{\numberline {5.4.1}Two-Stage Detector: Domain Filtering + OOD Detection}{46}{}%
\contentsline {subsection}{\numberline {5.4.2}Relationship to Near, Far, and Adjacent OOD}{47}{}%
\contentsline {section}{\numberline {5.5}Experimental Validation}{47}{}%
\contentsline {subsection}{\numberline {5.5.1}Domain Bench: Single-Domain Datasets}{47}{}%
\contentsline {subsection}{\numberline {5.5.2}Experimental Setup}{48}{}%
\contentsline {subsection}{\numberline {5.5.3}Results and Analysis}{49}{}%
\contentsline {subsection}{\numberline {5.5.4}Detailed Results by Dataset}{50}{}%
\contentsline {subsection}{\numberline {5.5.5}Case Study: Colon Dataset}{51}{}%
\contentsline {subsection}{\numberline {5.5.6}Discussion}{52}{}%
\contentsline {subsubsection}{Rock Dataset as an Outlier}{52}{}%
\contentsline {section}{\numberline {5.6}Limitations and Future Work}{53}{}%
\contentsline {subsection}{\numberline {5.6.1}Assumptions and Scope}{53}{}%
\contentsline {subsection}{\numberline {5.6.2}Generalization to Other Domains}{53}{}%
\contentsline {subsection}{\numberline {5.6.3}Alternative Solutions}{53}{}%
\contentsline {section}{\numberline {5.7}Conclusion}{54}{}%
\contentsline {chapter}{\numberline {6}Hallucinations Through the Lens of Mutual Information and Representation Learning}{56}{}%
\contentsline {section}{\numberline {6.1}Introduction and Motivation}{56}{}%
\contentsline {section}{\numberline {6.2}Theoretical Framework}{57}{}%
\contentsline {subsection}{\numberline {6.2.1}Mutual Information in Language Generation}{57}{}%
\contentsline {subsection}{\numberline {6.2.2}Hallucination as Information Loss}{57}{}%
\contentsline {section}{\numberline {6.3}Proposed Research Methodology}{58}{}%
\contentsline {subsection}{\numberline {6.3.1}Mutual Information Estimation in Foundation Models}{58}{}%
\contentsline {subsubsection}{Neural Mutual Information Estimation}{58}{}%
\contentsline {subsubsection}{Variational Bounds}{58}{}%
\contentsline {subsubsection}{Kernel-Based Methods}{58}{}%
\contentsline {subsubsection}{Discrete Approximations}{58}{}%
\contentsline {subsubsection}{Contrastive Mutual Information Estimation (Proposed)}{59}{}%
\contentsline {subsection}{\numberline {6.3.2}Representation Learning Analysis}{60}{}%
\contentsline {subsubsection}{Layer-wise Information Flow}{60}{}%
\contentsline {subsubsection}{Attention Mechanism Analysis}{60}{}%
\contentsline {subsubsection}{Information Bottleneck Dynamics}{60}{}%
\contentsline {section}{\numberline {6.4}Experimental Design}{61}{}%
\contentsline {subsection}{\numberline {6.4.1}Datasets and Benchmarks}{61}{}%
\contentsline {subsubsection}{Factual Question Answering}{61}{}%
\contentsline {subsubsection}{Hallucination-Specific Benchmarks}{61}{}%
\contentsline {subsection}{\numberline {6.4.2}Model Analysis}{61}{}%
\contentsline {subsubsection}{Architecture Comparison}{61}{}%
\contentsline {subsubsection}{Scale Analysis}{61}{}%
\contentsline {subsubsection}{Contrastive MI Estimation Validation}{62}{}%
\contentsline {section}{\numberline {6.5}Expected Contributions}{62}{}%
\contentsline {subsection}{\numberline {6.5.1}Theoretical Contributions}{62}{}%
\contentsline {subsection}{\numberline {6.5.2}Empirical Contributions}{63}{}%
\contentsline {subsection}{\numberline {6.5.3}Practical Applications}{63}{}%
\contentsline {section}{\numberline {6.6}Challenges and Limitations}{63}{}%
\contentsline {subsection}{\numberline {6.6.1}Technical Challenges}{63}{}%
\contentsline {subsection}{\numberline {6.6.2}Theoretical Limitations}{63}{}%
\contentsline {section}{\numberline {6.7}Related Work and Positioning}{64}{}%
\contentsline {subsection}{\numberline {6.7.1}Information Theory in Natural Language Processing}{64}{}%
\contentsline {subsubsection}{Classical Information-Theoretic Approaches}{64}{}%
\contentsline {subsubsection}{Mutual Information in Representation Learning}{64}{}%
\contentsline {subsubsection}{Information Bottleneck Theory}{64}{}%
\contentsline {subsection}{\numberline {6.7.2}Hallucination Detection and Mitigation}{65}{}%
\contentsline {subsubsection}{Confidence-Based Methods}{65}{}%
\contentsline {subsubsection}{Consistency-Based Approaches}{65}{}%
\contentsline {subsubsection}{Mechanistic Approaches}{66}{}%
\contentsline {subsection}{\numberline {6.7.3}Representation Learning in Language Models}{66}{}%
\contentsline {subsubsection}{Probing Studies}{66}{}%
\contentsline {subsubsection}{Mechanistic Interpretability}{66}{}%
\contentsline {subsubsection}{Information Flow Analysis}{67}{}%
\contentsline {subsection}{\numberline {6.7.4}Contrastive Learning in NLP}{67}{}%
\contentsline {subsubsection}{Sentence and Document Representations}{67}{}%
\contentsline {subsubsection}{Mutual Information Estimation via Contrastive Learning}{67}{}%
\contentsline {subsection}{\numberline {6.7.5}Positioning and Novel Contributions}{68}{}%
\contentsline {subsubsection}{Theoretical Contributions}{68}{}%
\contentsline {subsubsection}{Methodological Innovations}{68}{}%
\contentsline {subsubsection}{Bridging Theory and Practice}{68}{}%
\contentsline {subsubsection}{Relationship to Existing Work}{69}{}%
\contentsline {section}{\numberline {6.8}Conclusion}{69}{}%
\contentsline {chapter}{\numberline {7}Research Timeline}{70}{}%
\contentsline {section}{\numberline {7.1}Overview}{70}{}%
\contentsline {section}{\numberline {7.2}Phase 1: Foundation and Method Development (Months 1-4)}{70}{}%
\contentsline {subsection}{\numberline {7.2.1}Month 1: Theoretical Framework}{70}{}%
\contentsline {subsection}{\numberline {7.2.2}Month 2: Contrastive MI Implementation}{71}{}%
\contentsline {subsection}{\numberline {7.2.3}Month 3: Baseline Methods and Comparison}{71}{}%
\contentsline {subsection}{\numberline {7.2.4}Month 4: Method Refinement}{71}{}%
\contentsline {section}{\numberline {7.3}Phase 2: Large-Scale Validation and Hallucination Detection (Months 5-8)}{71}{}%
\contentsline {subsection}{\numberline {7.3.1}Month 5: Foundation Model Analysis}{71}{}%
\contentsline {subsection}{\numberline {7.3.2}Month 6: Hallucination Correlation Studies}{72}{}%
\contentsline {subsection}{\numberline {7.3.3}Month 7: Detection System Development}{72}{}%
\contentsline {subsection}{\numberline {7.3.4}Month 8: Cross-Architecture Validation}{72}{}%
\contentsline {section}{\numberline {7.4}Phase 3: Applications and Deployment (Months 9-12)}{72}{}%
\contentsline {subsection}{\numberline {7.4.1}Month 9: Domain-Specific Applications}{72}{}%
\contentsline {subsection}{\numberline {7.4.2}Month 10: Intervention Strategies}{73}{}%
\contentsline {subsection}{\numberline {7.4.3}Month 11: Comprehensive Evaluation}{73}{}%
\contentsline {subsection}{\numberline {7.4.4}Month 12: Documentation and Dissemination}{73}{}%
\contentsline {section}{\numberline {7.5}Key Deliverables}{73}{}%
\contentsline {subsection}{\numberline {7.5.1}Technical Deliverables}{73}{}%
\contentsline {subsection}{\numberline {7.5.2}Research Outputs}{74}{}%
\contentsline {section}{\numberline {7.6}Risk Mitigation}{74}{}%
\contentsline {subsection}{\numberline {7.6.1}Technical Risks}{74}{}%
\contentsline {subsection}{\numberline {7.6.2}Timeline Risks}{74}{}%
\contentsline {section}{\numberline {7.7}Success Metrics}{74}{}%
\contentsline {subsection}{\numberline {7.7.1}Quantitative Metrics}{74}{}%
\contentsline {subsection}{\numberline {7.7.2}Qualitative Metrics}{75}{}%
\contentsline {chapter}{\numberline {8}Discussion}{76}{}%
\contentsline {chapter}{Appendices}{89}{}%
\contentsline {chapter}{\numberline {A}Theoretical Proofs for Label Blindness}{90}{}%
\contentsline {section}{\numberline {A.1}Properties of Mutual Information and Entropy}{90}{}%
\contentsline {section}{\numberline {A.2}Supporting Theorems and Proofs}{91}{}%
\contentsline {subsection}{\numberline {A.2.1}Sufficiency}{91}{}%
\contentsline {subsection}{\numberline {A.2.2}Lower Bound of Mutual Information for Sufficiency}{92}{}%
\contentsline {subsection}{\numberline {A.2.3}Conditional Mutual Information of Noise}{93}{}%
\contentsline {subsection}{\numberline {A.2.4}Factorization of Bottleneck Loss}{94}{}%
\contentsline {section}{\numberline {A.3}Main Theorems and Proofs}{95}{}%
\contentsline {subsection}{\numberline {A.3.1}Strict Label Blindness in the Minimal Sufficient Statistic}{95}{}%
\contentsline {subsection}{\numberline {A.3.2}Independence of Filtered Distributions}{100}{}%
\contentsline {subsection}{\numberline {A.3.3}Strict Label Blindness in Filtered Distributions - Guaranteed OOD Failure}{101}{}%
\contentsline {subsection}{\numberline {A.3.4}Unavoidable Risk of Overlapping Out of Distribution Data}{103}{}%
